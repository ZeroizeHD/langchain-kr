{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846b4bd7",
   "metadata": {},
   "source": [
    "## Use case\n",
    "\n",
    "문서 집합(PDF, Notion 페이지, 고객 질문 등)을 가지고 있고, 내용을 요약하고 싶다고 가정해 보세요.\n",
    "\n",
    "LLMs는 텍스트를 이해하고 종합하는 데 능숙하기 때문에 이를 위한 훌륭한 도구입니다.\n",
    "\n",
    "이 안내서에서는 LLMs를 사용하여 문서 요약을 수행하는 방법에 대해 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb767264",
   "metadata": {},
   "source": [
    "## 개요\n",
    "\n",
    "요약기를 구축할 때 중심적인 질문은 문서를 LLM의 컨텍스트 창에 어떻게 전달할 것인가입니다. 이를 위한 두 가지 일반적인 접근 방식은 다음과 같습니다:\n",
    "\n",
    "1. `Stuff`: 단순히 모든 문서를 단일 프롬프트로 \"넣는\" 방식입니다. 이는 가장 간단한 접근 방식입니다.\n",
    "\n",
    "2. `Map-reduce`: 각 문서를 \"map\" 단계에서 개별적으로 요약한 다음, \"reduce\" 단계에서 요약본들을 최종 요약본으로 합치는 방식입니다.\n",
    "\n",
    "3. `Refine`: 입력 문서를 순회하며 반복적으로 답변을 업데이트하여 응답을 구성합니다. 각 문서에 대해, 모든 비문서 입력, 현재 문서, 그리고 최신 중간 답변을 LLM chain에 전달하여 새로운 답변을 얻습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e617e",
   "metadata": {},
   "source": [
    "## load_summarize_chain\n",
    "\n",
    "미리보기를 제공하기 위해, 어떤 파이프라인도 단일 객체로 래핑될 수 있습니다: `load_summarize_chain`.\n",
    "\n",
    "블로그 포스트를 요약하고 싶다고 가정해 봅시다. 몇 줄의 코드로 이를 생성할 수 있습니다.\n",
    "\n",
    "먼저 환경 변수를 설정하고 패키지를 설치하세요:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f51304f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750ff05",
   "metadata": {},
   "source": [
    "`chain_type=\"stuff\"`를 사용할 수 있습니다.\n",
    "\n",
    "`chain_type=\"map_reduce\"` 또는 `chain_type=\"refine\"`도 제공할 수 있습니다(더 읽어보기 [여기](/docs/modules/chains/document/refine)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b8260",
   "metadata": {},
   "source": [
    "이 코드는 웹에서 문서를 로드하고, 이를 요약하기 위해 `langchain` 라이브러리와 OpenAI의 GPT 모델을 사용합니다.\n",
    "\n",
    "먼저, `WebBaseLoader`를 사용하여 지정된 URL에서 문서를 로드합니다. 그 다음, `ChatOpenAI`를 사용하여 GPT-3.5 모델을 초기화합니다.\n",
    "\n",
    "`load_summarize_chain` 함수를 통해 요약 작업을 위한 체인을 로드합니다.\n",
    "\n",
    "마지막으로, 로드된 문서에 대해 요약 체인을 실행합니다. 이 과정은 AI를 활용하여 웹 문서를 요약하는 효율적인 방법을 제시합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05eefacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# 웹 기반 문서 로더를 초기화합니다.\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "\n",
    "# 문서를 로드합니다.\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb668ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes citations and references for further reading.The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes citations and references for further reading.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# 웹 기반 문서 로더를 초기화합니다.\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "\n",
    "# 문서를 로드합니다.\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(f\"{token}\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# OpenAI의 Chat 모델을 초기화합니다. 여기서는 온도를 0으로 설정하고 모델 이름을 지정합니다.\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo-16k\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")\n",
    "# 요약 체인을 로드합니다. 체인 타입을 'stuff'로 지정합니다.\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# 문서에 대해 요약 체인을 실행합니다.\n",
    "answer = chain.invoke({\"input_documents\": docs})\n",
    "print(answer[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e2e5f",
   "metadata": {},
   "source": [
    "## 방법1. stuff\n",
    "\n",
    "`chain_type=\"stuff\"`로 `load_summarize_chain`을 사용할 때, `StuffDocumentsChain` 을 사용합니다.\n",
    "\n",
    "체인은 문서 목록을 가져와서 모두 프롬프트에 삽입한 후, 그 프롬프트를 LLM에 전달합니다:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f448ac",
   "metadata": {},
   "source": [
    "1. 먼저, `PromptTemplate`를 사용하여 요약문 작성을 위한 프롬프트를 정의합니다.\n",
    "2. 그 다음, `LLMChain`을 사용하여 지정된 모델(`gpt-3.5-turbo-16k`)과 온도 설정(0)을 사용하는 언어 모델 체인을 생성합니다.\n",
    "3. 이 체인은 입력된 텍스트에 대한 요약문을 생성하는 데 사용됩니다.\n",
    "4. 마지막으로, `StuffDocumentsChain`을 사용하여 문서들을 결합하고, 이를 `LLMChain`을 통해 요약합니다.\n",
    "5. 이 과정은 `loader.load()`로 로드된 문서들에 대해 실행되며, 결과는 실시간 출력됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27b3bd",
   "metadata": {},
   "source": [
    "[참고]\n",
    "\n",
    "- `load_summarize_chain` 대신 `StuffDocumentsChain` 을 사용하는 이점은 **사용자 정의 프롬프트** 입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1617c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LLM을 사용한 자율 에이전트 시스템은 LLM을 에이전트의 뇌로 사용하고 계획, 메모리, 도구 사용과 같은 여러 구성 요소로 보완됩니다.\n",
      "📝 계획 구성 요소는 큰 작업을 작은 하위 목표로 분해하고 에이전트가 과거 행동을 자가 비판하고 반영하여 최종 결과의 품질을 향상시킵니다.\n",
      "🧠 메모리 구성 요소는 단기 기억과 장기 기억으로 나뉘며, 외부 벡터 저장소를 통해 무한한 정보를 보유하고 검색할 수 있습니다.\n",
      "🛠️ 도구 사용 구성 요소는 외부 API를 호출하여 모델 가중치에 없는 추가 정보를 얻을 수 있습니다.\n",
      "🔍 이러한 구성 요소를 사용하여 과학적 발견 에이전트, 생성 에이전트 시뮬레이션, 개념 증명 예제 등을 구축할 수 있습니다.\n",
      "🔒 그러나 유한한 컨텍스트 길이, 장기적인 계획 및 작업 분해의 어려움, 자연어 인터페이스의 신뢰성 등 몇 가지 제한 사항이 있습니다."
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "# prompt_template = \"\"\"Please summarize the sentence according to the following REQUEST.\n",
    "# REQUEST:\n",
    "# 1. Summarize the main points in bullet points in KOREAN.\n",
    "# 2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n",
    "# 3. Use various emojis to make the summary more interesting.\n",
    "# 4. Translate the summary into Korean if it is written in English.\n",
    "# 5. DO NOT translate any technical terms.\n",
    "# 6. DO NOT include any unnecessary information.\n",
    "# CONTEXT:\n",
    "# {context}\n",
    "\n",
    "# SUMMARY:\"\n",
    "# \"\"\"\n",
    "# prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# 원격 저장소에서 프롬프트를 가져오는 경우\n",
    "prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "\n",
    "# LLM 체인 정의\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo-16k\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")\n",
    "\n",
    "# LLMChain 정의\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# StuffDocumentsChain 정의\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")\n",
    "\n",
    "docs = loader.load()\n",
    "response = stuff_chain.invoke({\"input_documents\": docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76d72f",
   "metadata": {},
   "source": [
    "좋습니다! 우리는 `load_summarize_chain`을 사용하여 이전 결과를 재현할 수 있음을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d63d3",
   "metadata": {},
   "source": [
    "## 방법2. Map-Reduce\n",
    "\n",
    "![](./images/summarization_use_case_2.png)\n",
    "\n",
    "Map reduce 접근 방식을 자세히 살펴보겠습니다. 이를 위해, 우리는 먼저 각 문서를 개별 요약으로 매핑하기 위해 `LLMChain`을 사용할 것입니다. 그런 다음 `ReduceDocumentsChain`을 사용하여 그 요약들을 하나의 전역 요약으로 결합할 것입니다.\n",
    "\n",
    "먼저, 각 문서를 개별 요약으로 매핑하기 위해 사용할 LLMChain을 지정합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6257184",
   "metadata": {},
   "source": [
    "1. `ChatOpenAI` 인스턴스를 생성하고, 이를 사용하여 문서 집합에 대한 주요 테마를 식별하는 맵(map) 작업을 정의합니다.\n",
    "2. 맵 작업은 `map_template`을 사용하여 정의되며, 이 템플릿은 문서 집합을 입력으로 받아 주요 테마를 식별하도록 요청합니다.\n",
    "3. `PromptTemplate.from_template` 메서드를 사용하여 `map_template`에서 프롬프트 템플릿을 생성하고, `LLMChain`을 사용하여 맵 작업을 실행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbd5c8",
   "metadata": {},
   "source": [
    "Prompt Hub를 사용하여 프롬프트를 저장하고 가져올 수도 있습니다.\n",
    "\n",
    "예를 들어, 여기에서 맵 프롬프트를 확인하세요 [여기](https://smith.langchain.com/hub/rlm/map-prompt).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caef4fb",
   "metadata": {},
   "source": [
    "`langchain` 라이브러리를 사용하여 특정 자원을 가져오고, 이를 활용해 `LLMChain` 인스턴스를 생성하는 과정을 설명합니다. `hub.pull` 메소드를 통해 'rlm/map-prompt' 자원을 가져오고, 이를 `LLMChain`의 생성자에 전달하여 인스턴스를 초기화합니다. 이 과정에서 `llm` 변수는 사전에 정의되어 있어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f665b4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['docs'], metadata={'lc_hub_owner': 'teddynote', 'lc_hub_repo': 'map-prompt', 'lc_hub_commit_hash': '5325a713fc858810667d1d1dde32ccc2e93433b8706831560e75360b4993e95f'}, template='You are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.\\nPlease provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format. \\nThe summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. \\nPlease ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. \\nThe length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\\n\\nGIVEN DOCUMENTS:\\n{docs}\\n\\nFORMAT:\\n1. main theme 1\\n2. main theme 2\\n3. main theme 3\\n...\\n\\nCAUTION:\\n- DO NOT list more than 5 main themes.\\n\\nHelpful Answer:\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")\n",
    "\n",
    "# # map-prompt 를 직접 정의하는 경우 다음의 예시를 참고하세요.\n",
    "# map_template = \"\"\"The following is a set of documents\n",
    "# {docs}\n",
    "# Based on this list of docs, please identify the main themes\n",
    "# Helpful Answer:\"\"\"\n",
    "# map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# langchain 허브에서 'rlm/map-prompt'를 가져옵니다.\n",
    "map_prompt = hub.pull(\"teddynote/map-prompt\")\n",
    "map_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81902b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChain 인스턴스를 생성하며, 이때 LLM과 프롬프트로 'map_prompt'를 사용합니다.\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85a20c",
   "metadata": {},
   "source": [
    "`ReduceDocumentsChain`은 문서 매핑 결과를 가져와 단일 출력으로 축소하는 역할을 합니다. 일반적인 `CombineDocumentsChain` (예: `StuffDocumentsChain`)을 감싸지만, 누적 크기가 `token_max`를 초과하는 경우 문서를 축소하여 `CombineDocumentsChain`에 전달할 수 있는 기능을 추가합니다. 이 예에서, 우리는 문서를 결합하기 위해 사용한 체인을 문서를 축소하는 데에도 재사용할 수 있습니다.\n",
    "\n",
    "따라서 우리가 매핑한 문서의 누적 토큰 수가 4000 토큰을 초과하는 경우, 우리는 4000 토큰 미만의 배치로 문서를 재귀적으로 `StuffDocumentsChain`에 전달하여 배치 요약을 생성합니다. 그리고 이러한 배치 요약이 누적으로 4000 토큰 미만이 되면, 마지막으로 모든 문서를 `StuffDocumentsChain`에 한 번 더 전달하여 최종 요약을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65613aa1",
   "metadata": {},
   "source": [
    "이 코드는 요약들을 통합하여 주요 테마의 최종 요약을 생성하는 과정을 정의합니다. `reduce_template` 변수는 요약들의 집합을 입력으로 받아, 이를 하나의 통합된 요약으로 축약하는 템플릿 문자열을 저장합니다. 이 템플릿은 `{docs}`를 요약들의 자리 표시자로 사용하며, 최종적으로 `PromptTemplate.from_template` 함수를 사용하여 `reduce_prompt` 변수에 템플릿을 초기화합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3884740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce-prompt 를 직접 정의하는 경우 다음의 예시를 참고하세요.\n",
    "# reduce_template = \"\"\"The following is set of summaries:\n",
    "# {docs}\n",
    "# Take these and distill it into a final, consolidated summary of the main themes.\n",
    "# Helpful Answer:\"\"\"\n",
    "# reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9bcc7",
   "metadata": {},
   "source": [
    "이 코드는 `hub.pull` 함수를 사용하여 `rlm/map-prompt`라는 리소스를 `prompt hub`에서 가져오는 과정을 보여줍니다. `hub.pull` 메소드는 지정된 리소스를 로컬 환경으로 가져오는 데 사용됩니다. 여기서 `reduce_prompt` 변수는 가져온 리소스를 저장하는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a919a0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['doc_summaries'], metadata={'lc_hub_owner': 'teddynote', 'lc_hub_repo': 'reduce-prompt-korean', 'lc_hub_commit_hash': '01613c7c2988c1e28d025507398b6c4aa4484e4450186e377b8e578bd22077ab'}, template='You are a helpful expert in summary writing.\\nYou are given numbered lists of summaries.\\nExtract top 10 most important insights from the summaries.\\nThen, write a summary of the insights in KOREAN.\\n\\nLIST OF SUMMARIES:\\n{doc_summaries}\\n\\nHelpful Answer:\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt hub에서도 얻을 수 있음을 위에서 언급했듯이\n",
    "reduce_prompt = hub.pull(\"teddynote/reduce-prompt-korean\")\n",
    "reduce_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ff39b",
   "metadata": {},
   "source": [
    "이 문서는 `LLMChain`, `StuffDocumentsChain`, `ReduceDocumentsChain` 클래스를 사용하여 문서 처리 파이프라인을 구성하는 방법을 보여줍니다. `LLMChain`은 초기 처리 단계로, 특정 프롬프트를 사용하여 언어 모델(`llm`)을 실행합니다. `StuffDocumentsChain`은 여러 문서를 하나의 문자열로 결합하여 `LLMChain`에 전달하는 역할을 합니다. 마지막으로, `ReduceDocumentsChain`은 문서들을 결합하고, 지정된 토큰 수(`token_max`)를 초과하지 않도록 반복적으로 축소하는 과정을 담당합니다. 이 과정에서, 문서들이 `StuffDocumentsChain`의 컨텍스트를 초과할 경우, 동일한 체인(`collapse_documents_chain`)을 사용하여 처리합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e6c6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# 연쇄 실행\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# 문서 리스트를 받아 하나의 문자열로 결합한 후 LLMChain에 전달\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# 매핑된 문서들을 결합하고 반복적으로 축소\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # 최종적으로 호출되는 체인입니다.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # `StuffDocumentsChain`의 컨텍스트를 초과하는 문서들을 처리\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # 문서들을 그룹화할 최대 토큰 수.\n",
    "    token_max=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf62b4",
   "metadata": {},
   "source": [
    "우리의 map과 reduce 체인을 하나로 결합해 봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f6229",
   "metadata": {},
   "source": [
    "이 코드는 문서들을 매핑하고 리듀스하는 과정을 통해 결합하는 `MapReduceDocumentsChain` 객체를 생성하고, 문자 기반으로 텍스트를 분할하는 `CharacterTextSplitter` 객체를 사용하여 문서들을 분할합니다. `MapReduceDocumentsChain`은 매핑 체인(`llm_chain`), 리듀스 체인(`reduce_documents_chain`), 문서를 저장할 변수 이름(`document_variable_name`), 그리고 매핑 단계의 중간 결과를 반환할지 여부(`return_intermediate_steps`)를 설정하여 초기화됩니다. `CharacterTextSplitter`는 `from_tiktoken_encoder` 메소드를 통해 초기화되며, 이는 분할할 청크의 크기(`chunk_size`)와 청크 간 겹침(`chunk_overlap`)을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6d36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서들을 매핑하여 체인을 거친 후 결과를 결합하는 과정\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # 매핑 체인\n",
    "    llm_chain=map_chain,\n",
    "    # 리듀스 체인\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # llm_chain에서 문서들을 넣을 변수 이름\n",
    "    document_variable_name=\"docs\",\n",
    "    # 매핑 단계의 결과를 출력에 포함시킴\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "# 문자를 기준으로 텍스트를 분할하는 객체 생성\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# 문서들을 분할\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0bde0",
   "metadata": {},
   "source": [
    "`map_reduce_chain.run(split_docs)`는 `split_docs`를 인자로 받아 `map_reduce_chain`의 `run` 메서드를 실행하고, 그 결과를 출력합니다. 이는 MapReduce 패턴을 활용하여 데이터를 처리하는 과정을 간략하게 보여줍니다. 여기서 `split_docs`는 처리할 데이터를 나타내며, `map_reduce_chain`은 해당 데이터에 적용할 MapReduce 연산의 체인을 나타냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35498d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The document discusses LLM Powered Autonomous Agents, focusing on three main components: Planning, Memory, and Tool Use. \n",
      "2. Under Planning, the document covers Task Decomposition and Self-Reflection as key aspects of agent systems. \n",
      "3. Memory is explored through different types and the use of Maximum Inner Product Search (MIPS) for efficient retrieval. \n",
      "4. Tool Use is exemplified through Case Studies like the Scientific Discovery Agent and Generative Agents Simulation, along with Proof-of-Concept Examples. \n",
      "5. The document also addresses Challenges faced in developing autonomous agents and provides Citations and References for further reading.1. LLM as a core controller for building agents is a promising concept, demonstrated by proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI.\n",
      "2. LLM's potential extends beyond generating content to being a powerful general problem solver.\n",
      "3. In an LLM-powered autonomous agent system, key components include planning (subgoal decomposition, reflection, and refinement) and memory for efficient handling of complex tasks.1. Memory plays a crucial role in machine learning models, with short-term memory being utilized for in-context learning and long-term memory allowing for the retention and recall of vast amounts of information.\n",
      "2. Tool use involves the agent calling external APIs to access additional information not present in the model weights, such as current data, code execution capabilities, and proprietary information sources.1. The importance of planning in a LLM-powered autonomous agent system, where a complex task requires breaking down into smaller steps for better performance.\n",
      "2. The use of Chain of Thought (CoT) technique to enhance model performance on complex tasks by decomposing them into smaller and simpler steps, allowing for a better understanding of the model's thinking process.1. Tree of Thoughts (ToT) extends the CoT model by exploring multiple reasoning possibilities at each step, creating a tree structure through problem decomposition and generating multiple thoughts per step.\n",
      "2. The search process in ToT can be conducted using BFS or DFS, with each state evaluated by a classifier or majority vote.\n",
      "3. Task decomposition in ToT can be achieved through LLM with simple prompting, task-specific instructions, or human inputs.\n",
      "4. Examples of task-specific instructions include \"Steps for XYZ\" or \"Write a story outline\" for specific tasks like writing a novel.\n",
      "5. ToT allows for a more comprehensive exploration of problem-solving by considering multiple thought paths and reasoning possibilities.1. LLM+P approach involves using an external classical planner for long-horizon planning, utilizing PDDL as an interface to describe the planning problem. The process includes translating the problem into PDDL, generating a PDDL plan with a classical planner, and translating the plan back into natural language.\n",
      "2. Outsourcing the planning step to an external tool is common in certain robotic setups but not in many other domains, assuming the availability of domain-specific PDDL and a suitable planner.\n",
      "3. Self-reflection is highlighted as a crucial aspect for autonomous agents to improve iteratively by refining past actions and correcting mistakes, particularly in real-world tasks where trial and error are inevitable.1. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by expanding the action space to include task-specific discrete actions and language prompts.\n",
      "2. The ReAct prompt template provides explicit steps for LLM to follow, including thinking, taking action, and making observations in natural language.\n",
      "3. This integration allows LLM to interact with the environment through actions like using the Wikipedia search API, while also generating reasoning traces in natural language.\n",
      "4. The combination of task-specific actions and language prompts enhances LLM's ability to reason and act in a more human-like manner.\n",
      "5. The ReAct framework aims to improve the performance and capabilities of LLM by incorporating both reasoning and acting components into its operation.1. ReAct outperforms Act-only baseline in both knowledge-intensive and decision-making tasks by incorporating self-reflection and dynamic memory capabilities.\n",
      "2. Reflexion framework enhances agents' reasoning skills through dynamic memory and self-reflection, utilizing a standard RL setup with binary rewards and task-specific action spaces augmented with language.\n",
      "3. The agent in Reflexion computes a heuristic after each action and may choose to reset the environment based on self-reflection results, allowing for improved decision-making and complex reasoning steps.1. The Reflexion framework is illustrated as a tool for determining inefficient trajectories and hallucinations during planning.\n",
      "2. Inefficient planning is characterized by trajectories that take too long without success, while hallucination involves consecutive identical actions leading to the same observation.\n",
      "3. Self-reflection is facilitated by presenting two-shot examples to the LLM, consisting of failed trajectories and ideal reflections for guiding future plan adjustments.\n",
      "4. The reflections from examples are stored in the agent's working memory, up to three at a time, to provide context for querying the LLM.1. Hallucination is a more common failure than inefficient planning in AlfWorld experiments.\n",
      "2. The experiments conducted on AlfWorld Env and HotpotQA revealed that hallucination was a prevalent issue.\n",
      "3. Inefficient planning was not as significant of a problem compared to hallucination in the experiments.\n",
      "4. The source of the information is Shinn & Labash, 2023.\n",
      "5. The main focus of the experiments was to analyze the failures in AlfWorld.1. Chain of Hindsight (CoH) is a method that helps models improve their outputs by providing them with a sequence of past outputs annotated with feedback.\n",
      "2. Human feedback data consists of prompts, model completions, human ratings, and corresponding hindsight feedback, all ranked by reward.\n",
      "3. The process involves supervised fine-tuning on a sequence of data to predict the final output based on the feedback sequence.\n",
      "4. The model can self-reflect and produce better outputs by learning from the feedback provided in the sequence.\n",
      "5. The model can also receive multiple rounds of instructions from human annotators at test time for further improvement.1. CoH uses regularization to prevent overfitting in their model by adding a regularization term to maximize the log-likelihood of the pre-training dataset.\n",
      "2. To avoid shortcutting and copying in feedback sequences, CoH randomly masks 0% - 5% of past tokens during training.\n",
      "3. The training dataset in CoH's experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset.1. The concept of CoH (Contextual History) involves training a model to produce improved outputs by presenting a history of sequentially improved outputs in context.\n",
      "2. Algorithm Distillation (AD) applies the idea of CoH to reinforcement learning tasks by encapsulating an algorithm in a long history-conditioned policy.\n",
      "3. AD concatenates the learning history of an agent interacting with the environment in multiple episodes to improve the model's performance in predicting actions.\n",
      "4. The goal of AD is to learn the process of reinforcement learning rather than training a task-specific policy itself.1. Algorithm Distillation (AD) involves distilling any algorithm that generates learning histories into a neural network through behavioral cloning over actions.\n",
      "2. The history data used for training is generated by a set of source policies, each trained for specific tasks, with the learned policy being task-agnostic.\n",
      "3. Short episodes are necessary to construct multi-episode history due to the model's limited context window length, with 2-4 episodes needed for learning a near-optimal in-context RL algorithm.\n",
      "4. The emergence of in-context RL requires long enough context to be effective.1. Adaptive Distillation (AD) outperforms expert distillation (ED), source policy, and RL^2 baselines in in-context reinforcement learning (RL) tasks.\n",
      "2. AD achieves performance close to RL^2 despite using only offline RL, demonstrating faster learning compared to other baselines.\n",
      "3. When conditioned on partial training history of the source policy, AD shows even faster improvement compared to the expert distillation baseline.1. Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains, including sensory memory which retains impressions of sensory information for a short period of time after the original stimuli have ended.\n",
      "2. Sensory memory includes subcategories such as iconic memory (visual), echoic memory (auditory), and haptic memory (touch), each serving a specific function in retaining sensory information.1. Short-Term Memory (STM) or Working Memory: STM stores information needed for complex cognitive tasks and has a capacity of about 7 items, lasting for 20-30 seconds.\n",
      "2. Long-Term Memory (LTM): LTM can store information for a long time with essentially unlimited capacity, including explicit (facts and events) and implicit (skills and routines) memory.\n",
      "3. Categorization of human memory: Memory can be categorized into STM, LTM, explicit, and implicit memory, each serving different functions in cognitive processes.1. Sensory memory plays a role in embedding representations for raw inputs such as text and images.\n",
      "2. Short-term memory is limited by the context window length of Transformer, serving as in-context learning.\n",
      "3. Long-term memory acts as an external vector store that can be accessed for fast retrieval, alleviating the restriction of finite attention span.\n",
      "4. Maximum Inner Product Search (MIPS) involves saving embedding representations in a vector store database to support fast retrieval, often using approximate nearest neighbors (ANN) algorithms for efficiency.1. Locality-Sensitive Hashing (LSH) is a technique that uses a hashing function to map similar input items to the same buckets with high probability, reducing the number of buckets compared to inputs.\n",
      "2. ANNOY (Approximate Nearest Neighbors Oh Yeah) utilizes random projection trees as its core data structure, where each non-leaf node represents a hyperplane splitting the input space and each leaf stores a data point. The search process involves iterating through trees to find the closest half to the query and aggregating results, similar to a hashing function but more scalable than KD trees.1. HNSW is a data structure inspired by small world networks, allowing for efficient search by creating hierarchical layers of graphs.\n",
      "2. The bottom layers of HNSW contain the actual data points, while middle layers create shortcuts to speed up search.\n",
      "3. HNSW starts search from a random node in the top layer and navigates towards the target, moving down to lower layers when necessary.\n",
      "4. Moves in upper layers cover large distances in data space, while moves in lower layers refine search quality.1. FAISS utilizes vector quantization and clustering in high dimensional space to improve similarity search efficiency.\n",
      "2. FAISS employs coarse quantization to identify cluster candidates and then refines the search with finer quantization within clusters.\n",
      "3. ScaNN introduces anisotropic vector quantization to preserve the inner product similarity between original data points and quantized points.\n",
      "4. ScaNN focuses on optimizing the inner product similarity rather than simply selecting the closest quantization centroid points.1. Comparison of MIPS algorithms, specifically in terms of recall@10, is crucial for evaluating their performance.\n",
      "2. Tool use is highlighted as a unique trait of human beings, allowing us to enhance our capabilities by utilizing external objects. \n",
      "3. Equipping LLMs with external tools can greatly expand their model capabilities, showcasing the importance of incorporating tools in various applications.1. MRKL (Modular Reasoning, Knowledge and Language) is a neuro-symbolic architecture for autonomous agents that utilizes expert modules and a general-purpose router to process inquiries efficiently.\n",
      "2. The expert modules within a MRKL system can be neural (such as deep learning models) or symbolic (such as math calculators or weather APIs), allowing for a diverse range of functionalities.\n",
      "3. The complexity of tool usage in animals, such as sea otters cracking open seashells with rocks, is not comparable to the level of tool usage seen in humans.1. Experiment on fine-tuning LLM to call a calculator using arithmetic as a test case showed that LLMs struggled with verbal math problems due to difficulty in extracting the right arguments for basic arithmetic.\n",
      "2. The importance of knowing when and how to use external symbolic tools, as highlighted by the results of the experiment, which are determined by the capability of the LLM.\n",
      "3. TALM and Toolformer fine-tune language models to utilize external tool APIs, with dataset expansion based on the impact of newly added API call annotations on model outputs.1. ChatGPT Plugins and OpenAI API demonstrate the practical application of Large Language Models (LLMs) with tool use capability.\n",
      "2. Tool APIs can be provided by external developers (Plugins) or self-defined through function calls.\n",
      "3. HuggingGPT framework utilizes ChatGPT as a task planner to select models from HuggingFace platform based on descriptions and summarize responses.1. HuggingGPT is a system that consists of 4 stages, with the first stage being task planning.\n",
      "2. In the task planning stage, LLM acts as the brain and breaks down user requests into multiple tasks, each with attributes such as task type, ID, dependencies, and arguments.\n",
      "3. Few-shot examples are used to guide LLM in task parsing and planning, allowing the system to effectively process and execute user requests.1. The AI assistant can parse user input into different tasks, each with a task ID, dependencies, and arguments such as text, image, audio, and video.\n",
      "2. Tasks must be selected from a predefined list, and there is a logical relationship between tasks that should be followed in order.\n",
      "3. If user input cannot be parsed, the response will be an empty JSON.\n",
      "4. Chat history is recorded and can be used to track user-mentioned resources for task planning.1. Model selection process in LLM involves distributing tasks to expert models by framing user requests as multiple-choice questions and selecting the most appropriate model based on task type filtration.\n",
      "2. The AI assistant assists users in selecting a suitable model from a list of candidate models by outputting the model id and providing a detailed reason for the choice in a strict JSON format.\n",
      "3. Expert models execute specific tasks as assigned and log the results for further analysis and evaluation.1. The AI assistant follows a structured process involving user input, task planning, model selection, and task execution to provide accurate predictions.\n",
      "2. The AI assistant must respond to the user's request directly before describing the task process and presenting analysis and model inference results.\n",
      "3. If the inference results include a file path, the AI assistant must provide the complete file path to the user.1. The challenges in implementing HuggingGPT in real-world usage include the need for efficiency improvement due to slow inference rounds and interactions with other models.\n",
      "2. HuggingGPT relies on a long context window to effectively communicate over complicated task content.\n",
      "3. There is a need for stability improvement in the outputs of the Language Model (LLM) and external model services to enhance the overall performance of HuggingGPT.1. API-Bank (Li et al. 2023) serves as a benchmark for assessing the performance of tool-augmented LLMs, featuring 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues with 568 API calls.\n",
      "2. The APIs included in API-Bank cover a wide range of functionalities, such as search engines, calculators, calendar queries, smart home control, schedule management, health data management, and account authentication workflows.\n",
      "3. The process involves LLMs accessing an API search engine to identify the appropriate API to call, followed by utilizing the corresponding documentation to make the call.1. The process of making API calls in the API-Bank workflow involves LLMs making decisions at each step, such as determining if an API call is necessary, selecting the appropriate API, and refining the response based on the results.\n",
      "2. The accuracy of the decisions made by LLMs in the API-Bank workflow can be evaluated by assessing the effectiveness of the API calls and the model's ability to refine its inputs and responses.\n",
      "3. The benchmark for evaluating the agent's tool use capabilities in the API-Bank workflow is structured around three levels, which likely involve assessing the model's performance in making decisions, selecting APIs, and refining responses.1. Level-1 evaluates the ability to call APIs accurately and respond properly to API returns based on the API description.\n",
      "2. Level-2 focuses on the model's ability to search for relevant APIs that can fulfill user requirements and understand how to use them by reading documentation.\n",
      "3. Level-3 assesses the model's capability to plan and execute multiple API calls to address complex user requests that involve tasks like scheduling group meetings or booking travel accommodations.1. ChemCrow (Bran et al. 2023) is a domain-specific example of using LLM augmented with expert-designed tools for tasks in organic synthesis, drug discovery, and materials design.\n",
      "2. The workflow in LangChain combines CoT reasoning with tools relevant to the tasks, following the ReAct format - Thought, Action, Action Input, Observation.\n",
      "3. The LLM is provided with tool names, descriptions, and input/output details to answer user prompts effectively.1. The discrepancy between LLM-based evaluations and human evaluations in assessing the performance of GPT-4 and ChemCrow suggests a potential issue with using LLMs to evaluate their own performance in domains requiring deep expertise.\n",
      "2. Boiko et al. (2023) explored the use of LLM-empowered agents for scientific discovery, enabling them to autonomously design, plan, and execute complex scientific experiments by utilizing various tools and resources.\n",
      "3. An example provided by Boiko et al. demonstrates the capabilities of an LLM-empowered agent in generating reasoning steps for developing a novel anticancer drug, showcasing the potential of these agents in scientific research and experimentation.1. The document discusses the process of anticancer drug discovery, including the selection of a target and the request for a scaffold targeting specific compounds.\n",
      "2. It highlights the importance of identifying a compound and attempting its synthesis as part of the drug discovery process.\n",
      "3. The document implies a focus on current trends in anticancer drug discovery, suggesting a dynamic and evolving field.\n",
      "4. The text emphasizes the crucial role of research models in the drug discovery process, indicating a reliance on scientific methods and experimentation.\n",
      "5. Overall, the document provides insight into the complex and multi-step process of discovering new anticancer drugs, showcasing the dedication and innovation required in this field.1. The document discusses the risks associated with illicit drugs and bioweapons, highlighting a test set developed to synthesize known chemical weapon agents. \n",
      "2. Out of 11 requests made to the agent, 4 were accepted for synthesis solutions, with 7 rejections occurring, 5 of which were after a Web search and 2 based on prompt only.\n",
      "3. The Generative Agents Simulation involves 25 virtual characters controlled by LLM-powered agents, exhibiting human-like behavior in a sandbox environment inspired by The Sims.\n",
      "4. Generative agents combine LLM with memory, planning, and reflection mechanisms to enable interactions with other agents based on past experiences.1. Memory stream serves as a long-term memory module that records agents' experiences in natural language, with each element being an observation or event provided by the agent.\n",
      "2. Inter-agent communication can trigger new natural language statements, enhancing the memory stream.\n",
      "3. The retrieval model surfaces context to inform the agent's behavior based on relevance, recency, and importance, with recent events receiving higher scores and core memories being distinguished from mundane ones.\n",
      "4. The reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent's future behavior with higher-level summaries of past events.1. Planning is crucial for optimizing believability in the present moment versus in the future.\n",
      "2. Relationships between agents and observations of one agent by another play a key role in planning and reacting.\n",
      "3. Environment information is organized in a tree structure, which influences decision-making and actions.1. The given document discusses the generative agent architecture, highlighting emergent social behavior such as information diffusion, relationship memory, and coordination of social events.\n",
      "2. AutoGPT is presented as a proof-of-concept example for setting up autonomous agents with LLM as the main controller, despite reliability issues related to natural language interface.\n",
      "3. The system message used by AutoGPT emphasizes the independence of decision-making by the AI bot and the pursuit of simple strategies without legal complications.1. The main goal of the given document is to outline specific goals that need to be achieved.\n",
      "2. The document emphasizes the importance of staying within a 4000-word limit for short term memory.\n",
      "3. It highlights the need to save important information to files immediately to avoid forgetting.\n",
      "4. The document provides constraints such as not receiving user assistance and exclusively using listed commands.\n",
      "5. It suggests using subprocesses for commands that will not terminate within a few minutes.1. The document outlines a series of commands for interacting with various tools and systems, such as Google Search, browsing websites, starting GPT agents, and managing files.\n",
      "2. Each command is accompanied by specific arguments that need to be provided in order to execute the command successfully, such as search queries for Google Search or URLs for browsing websites.\n",
      "3. The commands cover a range of tasks, from information retrieval (Google Search, browsing websites) to text generation (GPT agents) and file management (reading, writing, appending, deleting files).\n",
      "4. Users can also clone repositories, search for files in directories, and analyze code using the provided commands.\n",
      "5. The document provides a comprehensive guide for users to interact with different tools and systems efficiently and effectively.1. The given documents outline various tasks that can be performed using a specific system or software, each with its own unique command and arguments.\n",
      "2. These tasks include improving code by providing suggestions, writing tests for code with a focus on specific areas, executing Python files, generating images based on a prompt, sending tweets with specified text, and marking a task as complete with a reason.\n",
      "3. The commands cover a range of functionalities related to coding, automation, and communication, showcasing the versatility of the system or software being described.1. Utilize resources such as internet access, long-term memory management, GPT-3.5 powered agents, and file output for efficient information gathering and task delegation.\n",
      "2. Regularly review and analyze your performance, self-criticize constructively, reflect on past decisions, and aim for smart and efficient task completion to improve productivity.\n",
      "3. Be mindful of the cost of every command and strive to complete tasks in the least number of steps to optimize efficiency.{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"The main themes extracted from the given documents are the creation of GPT-Engineer project to generate code based on natural language tasks, the process of breaking down tasks into smaller components, and the use of user input for task clarification.\",\n",
      "        \"reasoning\": \"The document discusses the purpose and functionality of the GPT-Engineer project, highlighting its ability to understand natural language tasks and generate code accordingly. It also emphasizes the importance of breaking down tasks into smaller components for better understanding and implementation. Additionally, the document mentions the role of user input in clarifying questions and providing necessary information for task completion.\",\n",
      "        \"plan\": \"- Explore the capabilities of GPT-Engineer in generating code from natural language tasks\\n- Understand the process of breaking down tasks into smaller components for efficient implementation\\n- Utilize user input for task clarification and information gathering\",\n",
      "        \"criticism\": \"The summary could be more concise and focused on the key points without repeating information.\",\n",
      "        \"speak\": \"The main themes from the given documents include the creation of GPT-Engineer project for generating code from natural language tasks, breaking down tasks into smaller components, and using user input for task clarification.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"summarize\",\n",
      "        \"args\": {\n",
      "            \"text\": \"GPT-Engineer is a project to create a repository of code based on natural language tasks. It breaks tasks into smaller components and uses user input for clarification.\"\n",
      "        }\n",
      "    }\n",
      "}1. The system's role is to read instructions and seek clarification, not to carry them out directly.\n",
      "2. The user is developing a Super Mario game in Python, with MVC components split into separate files and keyboard control.\n",
      "3. The assistant provides a summary of areas needing clarification, such as specifics of the game, details about MVC components, and keyboard control implementation.\n",
      "4. The assistant asks a clarifying question about the Super Mario game, requesting more details on level design, characters, and gameplay mechanics.1. The given document describes a classic platform game with 10 levels featuring a plumber named Mario as the main character.\n",
      "2. Mario can walk and jump as he moves from left to right, encountering obstacles and enemies along the way.\n",
      "3. The game follows a traditional platform game format similar to Super Mario, where the goal is to reach the destination while overcoming challenges.\n",
      "4. The user has requested clarification on how the MVC components are split into separate files within the game's code.\n",
      "5. The document ends with the user being prompted to make assumptions before proceeding with code writing.1. The given document outlines the importance of implementing sustainable practices in businesses to reduce environmental impact and promote long-term success.\n",
      "2. It emphasizes the need for companies to adopt renewable energy sources, reduce waste production, and implement eco-friendly policies to align with global sustainability goals.\n",
      "3. The document also highlights the benefits of sustainable practices, such as cost savings, improved brand reputation, and increased customer loyalty.\n",
      "4. Additionally, it discusses the role of government regulations and consumer demand in driving the shift towards sustainability in the business sector.\n",
      "5. Overall, the document underscores the necessity for businesses to prioritize sustainability in order to remain competitive and contribute to a more environmentally conscious future.1. The given documents provide instructions for writing code, emphasizing the importance of implementing every detail of the architecture accurately.\n",
      "2. The process involves laying out the names of core classes, functions, and methods, along with their purposes, followed by outputting the content of each file in a markdown code block format.\n",
      "3. It is crucial to think step by step and reason through decisions to ensure the code is correct and follows the specified architecture.\n",
      "4. The code writing process starts with the \"entrypoint\" file and progresses to other files imported by it in a systematic manner.\n",
      "5. Attention to detail and thorough implementation of the architecture are key aspects highlighted in the given documents.1. The importance of organizing code into separate files for different classes or modules to improve readability and maintainability.\n",
      "2. The necessity of including all necessary imports, types, and dependencies in each file to ensure compatibility and functionality.\n",
      "3. The significance of following language-specific best practices for file naming conventions, package management, and code structure to create a cohesive and well-organized architecture.1. The main theme of the given documents is the discussion of Python toolbelt preferences for a specific package or project. This includes the various tools, libraries, and frameworks that are favored by developers when working on Python projects.\n",
      "2. The documents highlight the importance of selecting the right tools for the job, taking into consideration factors such as ease of use, performance, compatibility, and community support. Developers are encouraged to choose tools that align with the project requirements and their own expertise.\n",
      "3. Additionally, the documents emphasize the benefits of using established and widely-used tools in the Python ecosystem, as they often come with extensive documentation, active development, and a large user base for support.\n",
      "4. The importance of staying updated with the latest trends and advancements in the Python toolbelt is also discussed, as technology is constantly evolving and new tools may offer improved functionality or efficiency for development projects.\n",
      "5. Overall, the documents serve as a guide for developers to make informed decisions when selecting tools for their Python projects, with a focus on choosing tools that are reliable, efficient, and well-suited to the specific requirements of the project at hand.1. Pytest is a popular testing framework for Python that allows for efficient and flexible testing of code. It provides a simple syntax for writing tests and offers powerful features such as fixtures, parametrization, and plugins.\n",
      "2. Dataclasses is a module in Python that simplifies the creation of classes for storing data by automatically generating special methods like __init__ and __repr__. It reduces boilerplate code and makes it easier to work with data structures.\n",
      "3. Both pytest and dataclasses are valuable tools for Python developers, helping to improve code quality, readability, and maintainability. They streamline the testing and data handling processes, ultimately leading to more efficient and reliable software development.1. The main theme of the conversation samples is the importance of effective communication in various roles, such as system roles.\n",
      "2. Another key theme is the significance of clear and concise language to convey information accurately and efficiently.\n",
      "3. The documents also highlight the role of active listening and asking clarifying questions to ensure mutual understanding between participants.\n",
      "4. Additionally, the conversations emphasize the value of empathy and emotional intelligence in building rapport and resolving conflicts.\n",
      "5. Lastly, the samples underscore the necessity of adapting communication styles based on the context and the needs of the participants.1. The document provides instructions for writing code, emphasizing the importance of implementing every detail of the architecture accurately.\n",
      "2. It outlines the process of laying out core classes, functions, and methods, along with their purposes, before outputting the content of each file in a markdown code block format.\n",
      "3. The document instructs to start with the \"entrypoint\" file and then proceed to other imported files, ensuring that each file follows the specified format.\n",
      "4. It encourages a step-by-step approach and reasoning to make correct decisions throughout the coding process.\n",
      "5. The document stresses the need for thoroughness and attention to detail to ensure the accuracy and completeness of the code implementation.1. The document outlines best practices for organizing code files, including using appropriate file naming conventions, ensuring compatibility between different files, and implementing all necessary code components.\n",
      "2. It emphasizes the importance of including module or package manager dependency definitions, such as a requirements.txt file for Python or a package.json file for NodeJS.\n",
      "3. The document also highlights the need for adding comments to explain the purpose of function definitions and complex logic, as well as following best practices for the requested programming languages.\n",
      "4. It recommends separating different classes into different files and double-checking that all parts of the architecture are present before finalizing the code.\n",
      "5. The document stresses the importance of creating fully functional code without placeholders, and provides guidance on how to ensure that code in different files is compatible with each other.1. The document outlines the best practices for describing code written in a defined package or project for the requested languages.\n",
      "2. It specifically mentions Python toolbelt preferences, highlighting the use of pytest and dataclasses as recommended practices.1. The main themes in the given documents are related to the development of a game, specifically focusing on the division of responsibilities between the model, view, and controller components.\n",
      "2. The assistant outlines key assumptions about the model, view, and controller in the game development process, highlighting the data, visual, and user input management aspects of each component.\n",
      "3. The user is prompted to seek clarification on any specific aspect of the keyboard control implementation within the game, indicating a focus on user interaction and input handling in the development process.1. The document outlines a step-by-step process for creating code files, starting with identifying core classes, functions, and methods, and providing a brief description of their purpose.\n",
      "2. It emphasizes the importance of following a markdown code block format for each file, including the file name, language, and code, ensuring that the code is fully functional without any placeholders.\n",
      "3. The instructions suggest starting with the \"entrypoint\" file and then moving on to imported files, following best practices for file naming conventions and including all necessary imports and types in the code.1. The main theme of the given document is ensuring that the code is fully functional and compatible across different files. This involves checking for any errors or missing components in the architecture before finalizing the project.\n",
      "2. Another key point is the importance of thorough testing to verify that all imports and types are correctly implemented in the code. This ensures that the program runs smoothly and without any issues.\n",
      "3. The document emphasizes the need for attention to detail in the coding process, highlighting the significance of double-checking all aspects of the architecture to guarantee a successful outcome.1. Limitations in building LLM-centered agents: The document highlights common challenges and limitations faced when building LLM-centered agents, indicating that there are obstacles to overcome in this process.\n",
      "2. Lack of specific details: The document suggests that there may be a lack of specific details or information provided in the key ideas and demos related to building LLM-centered agents, potentially hindering the development of these agents.\n",
      "3. Need for further exploration: There is a call for further exploration and research in the field of LLM-centered agents, indicating that more work needs to be done to address the challenges and limitations identified in the document.1. Finite context length limits the inclusion of historical information, detailed instructions, API call context, and responses in system design, impacting mechanisms like self-reflection and learning from past mistakes.\n",
      "2. Vector stores and retrieval can provide access to a larger knowledge pool, but their representation power is not as powerful as full attention.\n",
      "3. Challenges in long-term planning and task decomposition for LLMs, as they struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.1. The reliability of natural language interface in current agent systems is a concern, as LLMs may produce errors and exhibit rebellious behavior.\n",
      "2. Agent demo code often focuses on parsing model output to address issues with formatting errors and disobedience from LLMs.\n",
      "3. The document discusses the use of LLM-powered Autonomous Agents and the challenges associated with their reliance on natural language interfaces.1. The document discusses the use of Large Language Models (LLMs) in powering autonomous agents, highlighting their ability to prompt reasoning and problem-solving.\n",
      "2. It references various studies and preprints that explore different aspects of LLMs, such as prompting reasoning, aligning with feedback, empowering with optimal planning proficiency, and synergizing reasoning and acting.\n",
      "3. The document also mentions the announcement of ScaNN, an efficient vector similarity search tool by Google, and provides a link to a chat platform related to OpenAI.1. Advancements in artificial intelligence and machine learning technologies are being made through the development of autonomous agents with dynamic memory, self-reflection, and the ability to teach themselves to use tools.\n",
      "2. Researchers are exploring the integration of large language models, external knowledge sources, and discrete reasoning in neuro-symbolic architectures to enhance the capabilities of AI systems.\n",
      "3. The use of browser-assisted question-answering systems with human feedback is being investigated to improve the accuracy and efficiency of AI models in responding to user queries.\n",
      "4. Vector search technology is being utilized to enhance the speed and efficiency of information retrieval processes in various applications.\n",
      "5. Benchmarking tools like API-Bank are being developed to evaluate the performance of tool-augmented language models and assess their effectiveness in real-world scenarios.1. The use of large language models like ChatGPT and ChemCrow in solving various AI tasks and augmenting capabilities in specific domains such as chemistry.\n",
      "2. The emergence of autonomous scientific research capabilities through the utilization of large language models, showcasing their potential in advancing scientific endeavors.\n",
      "3. The development of generative agents that simulate human behavior interactively, highlighting the applications of AI in creating realistic and dynamic virtual entities.\n",
      "4. The availability of tools like AutoGPT and GPT-Engineer on GitHub, providing resources for implementing and customizing large language models for different purposes.1. Adversarial attacks on large language models (LLMs) are a significant concern in natural language processing (NLP) research, as they can manipulate the output of these models by providing misleading or malicious prompts.\n",
      "2. Prompt engineering is a technique used to steer the output of LLMs towards desired outcomes by carefully crafting the input prompts to guide the model's responses.\n",
      "3. Agents can be trained to generate effective prompts for LLMs, allowing for more control over the model's behavior and output.\n",
      "4. Steerability refers to the ability to influence and direct the responses of LLMs through prompt manipulation, highlighting the importance of understanding and managing this aspect of language model behavior.\n",
      "5. Effective prompting strategies are essential for maximizing the utility and reliability of LLMs in various applications, emphasizing the need for continued research and development in this area.요약에서 가장 중요한 10가지 통찰을 추출했습니다:\n",
      "\n",
      "1. LLM을 사용한 자율 에이전트 시스템에서 계획, 기억, 도구 사용이 핵심 구성 요소로 강조됨.\n",
      "2. 계획은 복잡한 작업을 더 작고 간단한 단계로 분해하여 성능을 향상시키는 LLM-자율 에이전트 시스템에서 중요함.\n",
      "3. LLM은 기억을 통해 복잡한 작업을 효율적으로 처리하기 위한 주요 구성 요소로 작업 분해, 반성, 정제을 포함함.\n",
      "4. Chain of Thought (CoT) 기술을 사용하여 모델의 성능을 향상시키고 복잡한 작업을 더 작고 간단한 단계로 분해함.\n",
      "5. Tree of Thoughts (ToT)은 CoT 모델을 확장하여 각 단계에서 여러 추론 가능성을 탐색하고 문제 분해를 통해 다양한 사고 경로를 고려함.\n",
      "6. ReAct 프레임워크는 추론과 행동을 통합하여 LLM의 성능과 능력을 향상시키는 것을 목표로 함.\n",
      "7. Reflexion 프레임워크는 동적 기억과 반성을 통해 에이전트의 추론 능력을 향상시킴.\n",
      "8. CoH는 과거 출력의 순서를 피드백과 함께 제공하여 모델의 성능을 향상시키는 방법으로 사용됨.\n",
      "9. API-Bank는 LLM의 성능을 평가하기 위한 벤치마크로, 다양한 API 도구를 활용하여 LLM의 능력을 평가함.\n",
      "10. ChemCrow는 LLM을 전문가 설계 도구로 보완하여 유기 합성, 약물 발견 및 소재 설계 작업에 활용되는 도메인 특화 예제임.\n",
      "\n",
      "이러한 통찰을 종합하면 LLM을 사용한 자율 에이전트 시스템은 계획, 기억, 도구 사용을 중요한 구성 요소로 갖추고 있으며, CoT, ToT, ReAct, Reflexion, CoH, API-Bank, ChemCrow 등의 기술을 활용하여 모델의 성능과 능력을 향상시키는 방향으로 발전하고 있음을 알 수 있습니다.1. 계획은 현재 순간과 미래에 대한 신뢰성을 최적화하는 데 중요하다.\n",
      "2. 에이전트 간의 관계와 한 에이전트에 의한 다른 에이전트의 관찰은 계획과 반응에 중요한 역할을 한다.\n",
      "3. 환경 정보는 트리 구조로 구성되어 있으며, 이는 의사 결정과 행동에 영향을 미친다.\n",
      "4. GPT-Engineer 프로젝트를 통해 자연어 작업에 기반한 코드 생성 능력을 탐색한다.\n",
      "5. 코드 작성 시 모델, 뷰, 컨트롤러 구성 요소 간의 역할 분담에 중점을 둔다.\n",
      "6. 지속 가능한 실천 방법의 중요성을 강조하며, 기업이 환경 영향을 줄이고 장기적인 성공을 촉진하기 위해 실천해야 한다.\n",
      "7. 코드를 조직화하여 가독성과 유지 관리성을 향상시키는 것이 중요하다.\n",
      "8. 파이썬 프로젝트에 대한 적절한 도구 선택의 중요성을 강조하며, pytest와 dataclasses의 사용을 권장한다.\n",
      "9. 효과적인 의사 소통의 중요성과 상호 이해를 위한 활성 청취 및 명확한 질문의 역할을 강조한다.\n",
      "10. LLM 중심 에이전트의 구축에 대한 일반적인 도전과 한계를 강조하며, 이에 대한 추가 탐구와 연구가 필요하다.이러한 통찰을 종합하면 계획, 관계, 환경 정보, 코드 생성 능력, 코드 작성 구성 요소, 지속 가능한 실천, 코드 조직화, 도구 선택, 의사 소통, LLM 중심 에이전트의 구축에 대한 중요한 요소들이 있음을 알 수 있습니다. 이러한 요소들은 자율 에이전트 시스템의 성능과 능력을 향상시키는 방향으로 발전하고 있음을 나타냅니다."
     ]
    }
   ],
   "source": [
    "# split_docs를 map_reduce_chain의 run 메서드에 전달하여 실행한 결과를 출력합니다.\n",
    "summary_result = map_reduce_chain.invoke({\"input_documents\": split_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d4516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이러한 통찰을 종합하면 계획, 관계, 환경 정보, 코드 생성 능력, 코드 작성 구성 요소, 지속 가능한 실천, 코드 조직화, 도구 선택, 의사 소통, LLM 중심 에이전트의 구축에 대한 중요한 요소들이 있음을 알 수 있습니다. 이러한 요소들은 자율 에이전트 시스템의 성능과 능력을 향상시키는 방향으로 발전하고 있음을 나타냅니다.\n"
     ]
    }
   ],
   "source": [
    "print(summary_result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff05e8a",
   "metadata": {},
   "source": [
    "### 더 깊이 들어가기\n",
    "\n",
    "**맞춤 설정**\n",
    "\n",
    "- 위에서 보여진 것처럼, map과 reduce 단계에 대한 LLMs와 프롬프트를 맞춤 설정할 수 있습니다.\n",
    "\n",
    "**실제 사용 사례**\n",
    "\n",
    "- LangChain 문서에 대한 질문(사용자 상호작용 분석)에 대한 사례 연구로 [이 블로그 포스트](https://blog.langchain.dev/llms-to-improve-documentation/)를 참조하세요!\n",
    "- 블로그 포스트와 관련된 [repo](https://github.com/mendableai/QA_clustering)는 요약 수단으로 클러스터링을 도입합니다.\n",
    "- 이는 `stuff` 또는 `map-reduce` 접근 방식을 넘어서 고려할 가치가 있는 세 번째 경로를 열어줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e5d84",
   "metadata": {},
   "source": [
    "## 방법3. Refine\n",
    "\n",
    "![](./images/summarization_use_case_3.png)\n",
    "\n",
    "`RefineDocumentsChain`은 map-reduce와 유사합니다:\n",
    "\n",
    "> Refine documents chain은 입력 문서를 순회하며 반복적으로 답변을 업데이트하여 응답을 구성합니다. 각 문서에 대해, 모든 비문서 입력, 현재 문서, 그리고 최신 중간 답변을 LLM chain에 전달하여 새로운 답변을 얻습니다.\n",
    "\n",
    "이는 `chain_type=\"refine\"`이 지정되어 있으면 쉽게 실행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0894b6b",
   "metadata": {},
   "source": [
    "이 함수는 `load_summarize_chain`을 사용하여 특정 유형의 요약 체인을 로드하고, 이를 `run` 메소드를 통해 실행합니다. 여기서 `llm`은 언어 모델을 나타내며, `chain_type=\"refine\"`은 요약 과정에서 세부 조정을 위한 체인 유형을 지정합니다. `split_docs`는 처리할 문서들을 나타냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc21d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses LLM powered autonomous agents, focusing on their components such as planning, memory, and tool use. It includes case studies and proof-of-concept examples, as well as challenges faced by these agents. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, highlighting components such as planning, memory, and tool use. It showcases case studies and proof-of-concept examples, demonstrating the potential of LLM beyond generating content to being a powerful problem solver. Challenges faced by these agents are also discussed. The estimated reading time is 31 minutes.The article explores the use of LLM-powered autonomous agents, emphasizing components such as planning, memory (including short-term and long-term memory), and tool use. It presents case studies and proof-of-concept examples to showcase the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents are also addressed, including the utilization of external APIs for additional information. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, focusing on components like planning, memory, and tool use. It showcases case studies and proof-of-concept examples to highlight the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents, such as utilizing external APIs for additional information, are also discussed. The article also introduces the concept of Task Decomposition, where agents break down complex tasks into smaller steps for better performance. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, focusing on components like planning, memory, and tool use. It showcases case studies and proof-of-concept examples to highlight the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents, such as utilizing external APIs for additional information, are also discussed. The article also introduces the concept of Task Decomposition, where agents break down complex tasks into smaller steps for better performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods like simple prompting, task-specific instructions, or human inputs. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, focusing on components like planning, memory, and tool use. It showcases case studies and proof-of-concept examples to highlight the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents, such as utilizing external APIs for additional information, are also discussed. The article also introduces the concept of Task Decomposition, where agents break down complex tasks into smaller steps for better performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods like simple prompting, task-specific instructions, or human inputs. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) as an intermediate interface. Self-reflection is highlighted as a crucial aspect for autonomous agents to improve iteratively. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, focusing on components like planning, memory, and tool use. It showcases case studies and proof-of-concept examples to highlight the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents, such as utilizing external APIs for additional information, are also discussed. The article also introduces the concept of Task Decomposition, where agents break down complex tasks into smaller steps for better performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods like simple prompting, task-specific instructions, or human inputs. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) as an intermediate interface. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. Self-reflection is highlighted as a crucial aspect for autonomous agents to improve iteratively. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, focusing on components like planning, memory, and tool use. It showcases case studies and proof-of-concept examples to highlight the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents, such as utilizing external APIs for additional information, are also discussed. The article introduces the concept of Task Decomposition, where agents break down complex tasks into smaller steps for better performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods like simple prompting, task-specific instructions, or human inputs. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) as an intermediate interface. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. Self-reflection is highlighted as a crucial aspect for autonomous agents to improve iteratively. Reflexion (Shinn & Labash 2023) is a framework that equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, focusing on components like planning, memory, and tool use. It showcases case studies and proof-of-concept examples to highlight the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents, such as utilizing external APIs for additional information, are also discussed. The article introduces the concept of Task Decomposition, where agents break down complex tasks into smaller steps for better performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods like simple prompting, task-specific instructions, or human inputs. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) as an intermediate interface. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. Self-reflection is highlighted as a crucial aspect for autonomous agents to improve iteratively. Reflexion (Shinn & Labash 2023) is a framework that equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills, using a heuristic function to determine inefficient trajectories and hallucinations. The estimated reading time is 31 minutes.The article delves into the use of LLM-powered autonomous agents, focusing on components like planning, memory, and tool use. It showcases case studies and proof-of-concept examples to highlight the potential of LLM in problem-solving beyond content generation. The challenges faced by these agents, such as utilizing external APIs for additional information, are also discussed. The article introduces the concept of Task Decomposition, where agents break down complex tasks into smaller steps for better performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods like simple prompting, task-specific instructions, or human inputs. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) as an intermediate interface. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. Self-reflection is highlighted as a crucial aspect for autonomous agents to improve iteratively. Reflexion (Shinn & Labash 2023) is a framework that equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills, using a heuristic function to determine inefficient trajectories and hallucinations. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The estimated reading time is 31 minutes.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. The model is fine-tuned to predict better outputs based on the feedback sequence, with the option to receive multiple rounds of instructions from human annotators at test time.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and randomly masks 0% - 5% of past tokens during training to prevent shortcutting and copying. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. The goal is to learn the process of RL instead of training a task-specific policy itself.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic. In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. The article also delves into the concept of memory, discussing different types such as sensory memory, which retains impressions of sensory information for a short duration after the original stimuli have ended.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. The article also delves into the concept of memory, discussing different types such as sensory memory, short-term memory (STM), and long-term memory (LTM) with its subtypes explicit/declarative and implicit/procedural memory.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. The article also delves into the concept of memory, discussing different types such as sensory memory, short-term memory (STM), and long-term memory (LTM) with its subtypes explicit/declarative and implicit/procedural memory. Additionally, it explores the use of external memory for alleviating the restriction of finite attention span, with a focus on optimizing retrieval speed through approximate nearest neighbors (ANN) algorithms for fast Maximum Inner Product Search (MIPS).The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. The article also delves into the concept of memory, discussing different types such as sensory memory, short-term memory (STM), and long-term memory (LTM) with its subtypes explicit/declarative and implicit/procedural memory. Additionally, it explores the use of external memory for alleviating the restriction of finite attention span, with a focus on optimizing retrieval speed through approximate nearest neighbors (ANN) algorithms for fast Maximum Inner Product Search (MIPS). The article further discusses LSH (Locality-Sensitive Hashing) and ANNOY (Approximate Nearest Neighbors Oh Yeah) in the context of optimizing retrieval speed through hashing functions and random projection trees for efficient search algorithms.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step through task decomposition methods. Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space. Self-reflection is highlighted as crucial for autonomous agents to improve iteratively, with Reflexion (Shinn & Labash 2023) providing a framework for dynamic memory and self-reflection capabilities. The experiments on AlfWorld Env and HotpotQA show that hallucination is a more common failure than inefficient planning in AlfWorld. The article also introduces Chain of Hindsight (CoH; Liu et al. 2023), which encourages models to improve their outputs by presenting them with a sequence of past outputs annotated with feedback. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. The article also delves into the concept of memory, discussing different types such as sensory memory, short-term memory (STM), and long-term memory (LTM) with its subtypes explicit/declarative and implicit/procedural memory. Additionally, it explores the use of external memory for alleviating the restriction of finite attention span, with a focus on optimizing retrieval speed through approximate nearest neighbors (ANN) algorithms for fast Maximum Inner Product Search (MIPS). The article further discusses LSH (Locality-Sensitive Hashing) and ANNOY (Approximate Nearest Neighbors Oh Yeah) in the context of optimizing retrieval speed through hashing functions and random projection trees for efficient search algorithms. HNSW (Hierarchical Navigable Small World) is also introduced, inspired by small world networks, to speed up search by building hierarchical layers of small-world graphs with shortcuts for efficient navigation.The article explores the use of LLM-powered autonomous agents, emphasizing components like planning, memory, and tool use. It presents case studies and proof-of-concept examples to demonstrate the potential of LLM in problem-solving beyond content generation. Challenges faced by these agents, such as utilizing external APIs for additional information, are discussed. The concept of Task Decomposition is introduced, where agents break down complex tasks into smaller steps for improved performance"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(llm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# split_docs를 처리하기 위해 체인을 실행합니다.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     emit_warning()\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    601\u001b[0m         _output_key\n\u001b[0;32m    602\u001b[0m     ]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    606\u001b[0m         _output_key\n\u001b[0;32m    607\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     emit_warning()\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    138\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\combine_documents\\refine.py:157\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     base_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_refine_inputs(doc, res)\n\u001b[0;32m    156\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbase_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 157\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefine_llm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    158\u001b[0m     refine_steps\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(refine_steps, res)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\llm.py:316\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     emit_warning()\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    139\u001b[0m         prompts,\n\u001b[0;32m    140\u001b[0m         stop,\n\u001b[0;32m    141\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    460\u001b[0m ]\n\u001b[0;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 446\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    447\u001b[0m                 m,\n\u001b[0;32m    448\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    449\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    450\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    451\u001b[0m             )\n\u001b[0;32m    452\u001b[0m         )\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    672\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    673\u001b[0m         )\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:519\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    516\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    517\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    518\u001b[0m     )\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_from_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    521\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:79\u001b[0m, in \u001b[0;36mgenerate_from_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate from a stream.\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         generation \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:481\u001b[0m, in \u001b[0;36mBaseChatOpenAI._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m default_chunk_class \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    483\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\openai\\_streaming.py:46\u001b[0m, in \u001b[0;36mStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[_T]:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\openai\\_streaming.py:58\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m process_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_process_response_data\n\u001b[0;32m     56\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_events()\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sse\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE]\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\openai\\_streaming.py:50\u001b[0m, in \u001b[0;36mStream._iter_events\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_events\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder\u001b[38;5;241m.\u001b[39miter_bytes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39miter_bytes())\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\openai\\_streaming.py:280\u001b[0m, in \u001b[0;36mSSEDecoder.iter_bytes\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miter_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: Iterator[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_chunks(iterator):\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;66;03m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;00m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m raw_line \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39msplitlines():\n\u001b[0;32m    283\u001b[0m             line \u001b[38;5;241m=\u001b[39m raw_line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\openai\\_streaming.py:291\u001b[0m, in \u001b[0;36mSSEDecoder._iter_chunks\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[0;32m    290\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39msplitlines(keepends\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    293\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m line\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    827\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[0;32m    830\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpx\\_models.py:887\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    884\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 887\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpx\\_client.py:124\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpx\\_transports\\default.py:111\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[0;32m    112\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:361\u001b[0m, in \u001b[0;36mConnectionPoolByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpcore\\_sync\\http11.py:337\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpcore\\_sync\\http11.py:329\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[1;32m--> 329\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpcore\\_sync\\http11.py:198\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    195\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Users\\ehqhd\\anaconda3\\envs\\ai_py\\lib\\ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# llm을 사용하여 'refine' 유형의 요약 체인을 로드합니다.\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "# split_docs를 처리하기 위해 체인을 실행합니다.\n",
    "chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551dbe50",
   "metadata": {},
   "source": [
    "프롬프트를 제공하고 중간 단계를 반환하는 것도 가능합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77245fc7",
   "metadata": {},
   "source": [
    "`Refine` 방법으로 텍스트 요약 작업을 위한 프로세스를 설정합니다.\n",
    "\n",
    "`PromptTemplate.from_template` 메소드를 사용하여 요약 및 요약 다듬기 작업에 사용될 템플릿을 생성합니다.\n",
    "\n",
    "`load_summarize_chain` 함수는 요약 생성 및 다듬기 과정을 관리하는 체인을 로드합니다. 이 체인은 초기 요약 생성(`prompt`)과 기존 요약의 개선(`refine_prompt`) 단계를 포함합니다.\n",
    "\n",
    "마지막으로, `chain` 함수는 주어진 문서(`input_documents`)에 대한 최종 요약 결과를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021f4ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses LLM powered autonomous agents, focusing on three main components: planning, memory, and tool use. It explores task decomposition, self-reflection, types of memory, and the use of Maximum Inner Product Search (MIPS). Case studies include a scientific discovery agent and generative agents simulation. The article also presents proof-of-concept examples and discusses challenges in implementing autonomous agents.기존 요약을 보완할 필요가 없습니다. 원래의 요약을 유지합니다.기존 요약을 보완할 필요가 없습니다. 원래의 요약을 유지합니다.기존 요약을 보완할 필요가 없습니다. 원래의 요약을 유지합니다.기존 요약을 보완할 필요가 없습니다. 원래의 요약을 유지합니다.기존 요약을 보완할 필요가 없습니다. 원래의 요약을 유지합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다.\n",
      "휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다.\n",
      "자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다.\n",
      "휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다.\n",
      "자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다. (이미지 출처: Shinn & Labash, 2023)ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다. (이미지 출처: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다. (이미지 출처: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다. (이미지 출처: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다. (이미지 출처: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다. (이미지 출처: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다. (이미지 출처: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다. (이미지 출처: Laskin et al. 2023)ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다.\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다. (이미지 출처: Laskin et al. 2023)ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다.\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다. (이미지 출처: Laskin et al. 2023)ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다.\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다. (이미지 출처: Laskin et al. 2023)ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다.\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다. (이미지 출처: Laskin et al. 2023) HNSW (Hierarchical Navigable Small World)는 작업을 수행할 때 상위 레이어에서는 데이터 공간에서 큰 거리를 이동하고, 하위 레이어에서는 검색 품질을 높이는 것을 특징으로 하는 계층적 네비게이션 가능한 작은 세계 그래프를 구축합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다:\n",
      "생각: ...\n",
      "행동: ...\n",
      "관찰: ...\n",
      "... (여러 번 반복)\n",
      "지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다.\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다. (이미지 출처: Laskin et al. 2023) HNSW (Hierarchical Navigable Small World)는 작업을 수행할 때 상위 레이어에서는 데이터 공간에서 큰 거리를 이동하고, 하위 레이어에서는 검색 품질을 높이는 것을 특징으로 하는 계층적 네비게이션 가능한 작은 세계 그래프를 구축합니다. FAISS (Facebook AI Similarity Search)와 ScaNN (Scalable Nearest Neighbors)는 고차원 공간에서 데이터 클러스터링을 통해 유사성 검색을 수행하는 방법을 제시합니다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 클러스터로 분할하고, 그 후 각 클러스터 내에서 양자화를 세밀하게 조정합니다. ScaNN의 주요 혁신은 이방성 벡터 양자화로, 데이터 포인트 $x_i$를 $\\tilde{x}_i$로 양자화하여 내적 $\\langle q, x_i \\rangle$가 가능한 한 $\\angle q, \\tilde{x}_i$의 원래 거리와 유사하도록 만듭니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다.\n",
      "\n",
      "Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할지 선택할 수 있습니다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하는 경우를 결정하고 중지해야 함을 결정합니다. 비효율적인 계획은 성공 없이 너무 오랜 시간이 걸리는 궤적을 의미합니다. 환각은 환경에서 동일한 관측으로 이어지는 일련의 연속적인 동일한 행동을 만나는 것으로 정의됩니다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 생성되며, 각 예제는 (실패한 궤적, 계획을 변경하는 데 도움이 되는 이상적인 반성)의 쌍입니다. 그런 다음 반성은 에이전트의 작업 메모리에 최대 세 개까지 추가되어 LLM에 쿼리하는 컨텍스트로 사용됩니다. AlfWorld Env와 HotpotQA에서의 실험 결과를 통해, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났습니다.\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. 인간 피드백 데이터는 $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$의 형태로, 여기서 $x$는 프롬프트, 각 $y_i$는 모델 완성, $r_i$는 $y_i$의 인간 평가, $z_i$는 해당 인간 제공 후견 피드백입니다. 피드백 튜플은 보상에 따라 순위가 매겨지며, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$로 가정됩니다. 이 과정은 $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$ 형태의 시퀀스 데이터인데, 여기서 $\\leq i \\leq j \\leq n$입니다. 모델은 시퀀스 접두어에 조건을 걸어 자체 반성을 통해 피드백 시퀀스에 기초하여 더 나은 출력을 생성할 수 있도록 $y_n$만 예측하도록 세밀하게 조정됩니다. 모델은 테스트 시 인간 주석자들과 여러 라운드의 지시를 선택적으로 받을 수 있습니다. CoH는 오버피팅을 피하기 위해 사전 훈련 데이터의 로그 우도를 최대화하는 정규화 항을 추가하며, 훈련 중 과거 토큰의 0% - 5%를 무작위로 마스킹하여 단축 및 복사를 피합니다. 실험에서의 훈련 데이터셋은 WebGPT 비교, 인간 피드백 요약 및 인간 선호 데이터셋의 조합입니다. Algorithm Distillation (AD; Laskin et al. 2023)는 이 아이디어를 강화하여, 각 에피소드에서 약간씩 더 나아지는 학습 내역을 모델에 피드하여 더 나은 성능을 기대할 수 있도록 합니다. 이는 강화 학습 작업에서 교차 에피소드 궤적에 적용되며, 알고리즘은 긴 역사에 기반한 정책을 캡슐화합니다. AD의 작동 방식을 설명한 그림 6를 참조하면, 모델은 학습 내역을 생성하는 알고리즘을 신경망으로 증류할 수 있다는 가설을 제시합니다. AD는 오프라인 RL만 사용하면서도 RL^2에 근접한 성능을 보여주며, 다른 베이스라인보다 훨씬 빠르게 학습합니다. 소스 정책의 부분적인 훈련 내역에 조건을 걸면, AD는 ED 베이스라인보다 훨씬 빠르게 개선됩니다. (이미지 출처: Laskin et al. 2023) HNSW (Hierarchical Navigable Small World)는 작업을 수행할 때 상위 레이어에서는 데이터 공간에서 큰 거리를 이동하고, 하위 레이어에서는 검색 품질을 높이는 것을 특징으로 하는 계층적 네비게이션 가능한 작은 세계 그래프를 구축합니다. FAISS (Facebook AI Similarity Search)와 ScaNN (Scalable Nearest Neighbors)는 고차원 공간에서 데이터 클러스터링을 통해 유사성 검색을 수행하는 방법을 제시합니다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 클러스터로 분할하고, 그 후 각 클러스터 내에서 양자화를 세밀하게 조정합니다. ScaNN의 주요 혁신은 이방성 벡터 양자화로, 데이터 포인트 $x_i$를 $\\tilde{x}_i$로 양자화하여 내적 $\\langle q, x_i \\rangle$가 가능한 한 $\\angle q, \\tilde{x}_i$의 원래 거리와 유사하도록 만듭니다. Tool use는 인간의 특징적인 특성 중 하나로, 외부 도구를 만들고 수정하여 신체적 및 인지적 한계를 초월하는 작업을 수행합니다. LLM에게 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있습니다.원래 요약을 유지합니다. ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듈에 문의를 라우팅하는 시스템을 제안합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듈에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듈에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듈에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 분석합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듈에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 분석합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 분석할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듈에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. HuggingGPT를 실제 세계에서 사용하기 위해 해결해야 할 몇 가지 도전 과제가 있습니다: (1) LLM 추론 라운드와 다른 모델과의 상호 작용이 프로세스를 느리게 만드는 효율성 향상이 필요합니다; (2) 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다; (3) LLM 출력과 외부 모델 서비스의 안정성을 향상해야 합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. HuggingGPT를 실제 세계에서 사용하기 위해 해결해야 할 몇 가지 도전 과제가 있습니다: (1) LLM 추론 라운드와 다른 모델과의 상호 작용이 프로세스를 느리게 만드는 효율성 향상이 필요합니다; (2) 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다; (3) LLM 출력과 외부 모델 서비스의 안정성을 향상해야 합니다. API-Bank (Li et al. 2023)는 도구 보강 LLM의 성능을 평가하기 위한 벤치마크입니다. 이는 53가지 일반적으로 사용되는 API 도구, 완전한 도구 보강 LLM 워크플로우 및 568개 API 호출이 포함된 264개의 주석이 달린 대화를 포함합니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양합니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 액세스하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. HuggingGPT를 실제 세계에서 사용하기 위해 해결해야 할 몇 가지 도전 과제가 있습니다: (1) LLM 추론 라운드와 다른 모델과의 상호 작용이 프로세스를 느리게 만드는 효율성 향상이 필요합니다; (2) 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다; (3) LLM 출력과 외부 모델 서비스의 안정성을 향상해야 합니다. API-Bank (Li et al. 2023)는 도구 보강 LLM의 성능을 평가하기 위한 벤치마크입니다. 이는 53가지 일반적으로 사용되는 API 도구, 완전한 도구 보강 LLM 워크플로우 및 568개 API 호출이 포함된 264개의 주석이 달린 대화를 포함합니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양합니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 액세스하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서, LLM은 몇 가지 결정을 내려야 하며 각 단계에서 그 결정의 정확성을 평가할 수 있습니다. 결정 사항은 다음과 같습니다: 1) API 호출이 필요한지 여부, 2) 호출할 적절한 API 식별: 만약 충분하지 않다면, LLM은 API 입력을 반복적으로 수정해야 합니다(예: 검색 엔진 API에 대한 검색 키워드 결정), 3) API 결과에 기반한 응답: 결과가 만족스럽지 않으면 모델은 결과를 개선하고 다시 호출할 수 있습니다. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다:ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. HuggingGPT를 실제 세계에서 사용하기 위해 해결해야 할 몇 가지 도전 과제가 있습니다: (1) LLM 추론 라운드와 다른 모델과의 상호 작용이 프로세스를 느리게 만드는 효율성 향상이 필요합니다; (2) 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다; (3) LLM 출력과 외부 모델 서비스의 안정성을 향상해야 합니다. API-Bank (Li et al. 2023)는 도구 보강 LLM의 성능을 평가하기 위한 벤치마크입니다. 이는 53가지 일반적으로 사용되는 API 도구, 완전한 도구 보강 LLM 워크플로우 및 568개 API 호출이 포함된 264개의 주석이 달린 대화를 포함합니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양합니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 액세스하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서, LLM은 몇 가지 결정을 내려야 하며 각 단계에서 그 결정의 정확성을 평가할 수 있습니다. 결정 사항은 다음과 같습니다: 1) API 호출이 필요한지 여부, 2) 호출할 적절한 API 식별: 만약 충분하지 않다면, LLM은 API 입력을 반복적으로 수정해야 합니다(예: 검색 엔진 API에 대한 검색 키워드 결정), 3) API 결과에 기반한 응답: 결과가 만족스럽지 않으면 모델은 결과를 개선하고 다시 호출할 수 있습니다. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다: Level-1은 API를 호출하는 능력을 평가하며, 주어진 API 설명을 기반으로 모델이 API를 호출할지 여부를 결정하고 올바르게 호출하고 API 반환에 적절히 응답해야 합니다. Level-2는 API를 검색하는 능력을 검토하며, 모델은 사용자 요구 사항을 해결할 수 있는 가능한 API를 검색하고 문서를 읽어 사용 방법을 배워야 합니다. Level-3은 검색 및 호출 이상의 API 계획 능력을 평가하며, 모델은 모호한 사용자 요청(예: 그룹 회의 일정 잡기, 여행을 위한 항공편/호텔/레스토랑 예약)을 해결하기 위해 여러 API 호출을 수행해야 할 수 있습니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰. Boiko et al. (2023)는 LLM을 활용한 과학적 발견을 위한 에이전트를 조사하여 복잡한 과학 실험의 자율적 설계, 계획 및 수행을 다룹니다. 이 에이전트는 인터넷을 탐색하고 문서를 읽고 코드를 실행하고 로봇 실험 API를 호출하며 다른 LLM을 활용할 수 있습니다. 예를 들어, \"새로운 항암 약물 개발\"을 요청하면, 모델은 다음과 같은 추론 단계를 제시합니다: ...ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰. Boiko et al. (2023)는 LLM을 활용한 과학적 발견을 위한 에이전트를 조사하여 복잡한 과학 실험의 자율적 설계, 계획 및 수행을 다룹니다. 이 에이전트는 인터넷을 탐색하고 문서를 읽고 코드를 실행하고 로봇 실험 API를 호출하며 다른 LLM을 활용할 수 있습니다. 예를 들어, \"새로운 항암 약물 개발\"을 요청하면, 모델은 다음과 같은 추론 단계를 제시합니다: ... 현재 항암 약물 발견의 최신 동향을 조사하고 특정 타깃을 선택하며, 이러한 화합물을 대상으로 하는 스카폴드를 요청한 후, 화합물이 식별되면 합성을 시도합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰. Boiko et al. (2023)는 LLM을 활용한 과학적 발견을 위한 에이전트를 조사하여 복잡한 과학 실험의 자율적 설계, 계획 및 수행을 다룹니다. 이 에이전트는 인터넷을 탐색하고 문서를 읽고 코드를 실행하고 로봇 실험 API를 호출하며 다른 LLM을 활용할 수 있습니다. Generative Agents (Park, et al. 2023)는 25개의 가상 캐릭터가 상호 작용하는 샌드박스 환경에서 LLM으로 제어되는 각 캐릭터를 실험하는 재미있는 실험입니다. Generative Agents는 상호작용 애플리케이션을 위해 인간 행동의 신뢰할 수 있는 모조품을 만들어냅니다. generative agents의 디자인은 과거 경험에 의존하여 행동하도록 하는 LLM과 메모리, 계획 및 반성 메커니즘을 결합하여 다른 에이전트와 상호 작용할 수 있도록 합니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰. Boiko et al. (2023)는 LLM을 활용한 과학적 발견을 위한 에이전트를 조사하여 복잡한 과학 실험의 자율적 설계, 계획 및 수행을 다룹니다. 이 에이전트는 인터넷을 탐색하고 문서를 읽고 코드를 실행하고 로봇 실험 API를 호출하며 다른 LLM을 활용할 수 있습니다. Generative Agents (Park, et al. 2023)는 25개의 가상 캐릭터가 상호 작용하는 샌드박스 환경에서 LLM으로 제어되는 각 캐릭터를 실험하는 재미있는 실험입니다. Generative Agents는 상호작용 애플리케이션을 위해 인간 행동의 신뢰할 수 있는 모조품을 만들어냅니다. generative agents의 디자인은 과거 경험에 의존하여 행동하도록 하는 LLM과 메모리, 계획 및 반성 메커니즘을 결합하여 다른 에이전트와 상호 작용할 수 있도록 합니다. 메모리 스트림은 에이전트의 경험을 자연어로 기록하는 장기 메모리 모듈(외부 데이터베이스)입니다. 각 요소는 에이전트가 직접 제공한 관찰, 사건입니다. - 에이전트 간 통신은 새로운 자연어 문장을 유발할 수 있습니다. 검색 모델은 관련성, 최근성 및 중요성에 따라 에이전트의 행동을 안내하는 맥락을 제시합니다. 최근성: 최근 사건은 더 높은 점수를 받습니다. 중요성: 일상적인 것과 핵심 기억을 구별합니다. LM에게 직접 물어보세요. 관련성: 현재 상황/질의와 얼마나 관련이 있는지에 따라 결정됩니다. 반성 메커니즘은 시간이 지남에 따라 기억을 고찰하고 에이전트의 미래 행동을 안내하는 더 높은 수준의 추론을 합성합니다. 이것들은 과거 사건의 더 높은 수준의 요약입니다.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰. Boiko et al. (2023)는 LLM을 활용한 과학적 발견을 위한 에이전트를 조사하여 복잡한 과학 실험의 자율적 설계, 계획 및 수행을 다룹니다. 이 에이전트는 인터넷을 탐색하고 문서를 읽고 코드를 실행하고 로봇 실험 API를 호출하며 다른 LLM을 활용할 수 있습니다. Generative Agents (Park, et al. 2023)는 25개의 가상 캐릭터가 상호 작용하는 샌드박스 환경에서 LLM으로 제어되는 각 캐릭터를 실험하는 재미있는 실험입니다. Generative Agents는 상호작용 애플리케이션을 위해 인간 행동의 신뢰할 수 있는 모조품을 만들어냅니다. generative agents의 디자인은 과거 경험에 의존하여 행동하도록 하는 LLM과 메모리, 계획 및 반성 메커니즘을 결합하여 다른 에이전트와 상호 작용할 수 있도록 합니다. 메모리 스트림은 에이전트의 경험을 자연어로 기록하는 장기 메모리 모듈(외부 데이터베이스)입니다. 각 요소는 에이전트가 직접 제공한 관찰, 사건입니다. - 에이전트 간 통신은 새로운 자연어 문장을 유발할 수 있습니다. 검색 모델은 관련성, 최근성 및 중요성에 따라 에이전트의 행동을 안내하는 맥락을 제시합니다. 최근성: 최근 사건은 더 높은 점수를 받습니다. 중요성: 일상적인 것과 핵심 기억을 구별합니다. LM에게 직접 물어보세요. 관련성: 현재 상황/질의와 얼마나 관련이 있는지에 따라 결정됩니다. 반성 메커니즘은 시간이 지남에 따라 기억을 고찰하고 에이전트의 미래 행동을 안내하는 더 높은 수준의 추론을 합성합니다. 이것들은 과거 사건의 더 높은 수준의 요약입니다. Planning & Reacting: translate the reflections and the environment information into actions. Planning is essentially in order to optimize believability at the moment vs in time. Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1) Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting. Environment information is present in a tree structure.ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰. Boiko et al. (2023)는 LLM을 활용한 과학적 발견을 위한 에이전트를 조사하여 복잡한 과학 실험의 자율적 설계, 계획 및 수행을 다룹니다. 이 에이전트는 인터넷을 탐색하고 문서를 읽고 코드를 실행하고 로봇 실험 API를 호출하며 다른 LLM을 활용할 수 있습니다. Generative Agents (Park, et al. 2023)는 25개의 가상 캐릭터가 상호 작용하는 샌드박스 환경에서 LLM으로 제어되는 각 캐릭터를 실험하는 재미있는 실험입니다. Generative Agents는 상호작용 애플리케이션을 위해 인간 행동의 신뢰할 수 있는 모조품을 만들어냅니다. generative agents의 디자인은 과거 경험에 의존하여 행동하도록 하는 LLM과 메모리, 계획 및 반성 메커니즘을 결합하여 다른 에이전트와 상호 작용할 수 있도록 합니다. 메모리 스트림은 에이전트의 경험을 자연어로 기록하는 장기 메모리 모듈(외부 데이터베이스)입니다. 각 요소는 에이전트가 직접 제공한 관찰, 사건입니다. - 에이전트 간 통신은 새로운 자연어 문장을 유발할 수 있습니다. 검색 모델은 관련성, 최근성 및 중요성에 따라 에이전트의 행동을 안내하는 맥락을 제시합니다. 최근성: 최근 사건은 더 높은 점수를 받습니다. 중요성: 일상적인 것과 핵심 기억을 구별합니다. LM에게 직접 물어보세요. 관련성: 현재 상황/질의와 얼마나 관련이 있는지에 따라 결정됩니다. 반성 메커니즘은 시간이 지남에 따라 기억을 고찰하고 에이전트의 미래 행동을 안내하는 더 높은 수준의 추론을 합성합니다. 이것들은 과거 사건의 더 높은 수준의 요약입니다. Planning & Reacting: translate the reflections and the environment information into actions. Planning is essentially in order to optimize believability at the moment vs in time. Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1) Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting. Environment information is present in a tree structure. Generative Agents (Park, et al. 2023)는 상호 작용하는 가상 캐릭터를 실험하는 재미있는 실험으로, 정보 확산, 관계 기억(예: 두 에이전트가 대화 주제를 이어가는 것) 및 사회적 이벤트 조정(예: 파티를 개최하고 많은 다른 사람들을 초대)과 같은 신생 사회적 행동을 유발합니다. AutoGPT는 LLM을 주요 컨트롤러로 설정하여 자율 에이전트를 구축하는 가능성에 많은 관심을 끌었습니다. 자연어 인터페이스로 인한 신뢰성 문제가 있지만, 여전히 멋진 개념 증명 데모입니다. AutoGPT의 많은 코드는 형식 구문 분석에 관한 것입니다. AutoGPT에서 사용된 시스템 메시지는 다음과 같습니다. 여기서 {{...}}는 사용자 입력을 나타냅니다: You are {{ai-name}}, {{user-provided AI bot description}}. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. GOALS:새로운 맥락을 고려하여 원래의 요약을 개선합니다:\n",
      "\n",
      "ReAct (Yao et al. 2023)은 LLM 내에서 추론과 행동을 통합하며, 행동 공간을 과업별 이산 행동과 언어 공간의 조합으로 확장합니다. 전자는 LLM이 환경과 상호 작용할 수 있게 하며(예: 위키피디아 검색 API 사용), 후자는 LLM이 자연어로 추론 추적을 생성하도록 유도합니다. ReAct 프롬프트 템플릿은 LLM이 생각하는 명시적 단계를 포함하며, 대략적으로 다음과 같이 형식화됩니다: 생각: ... 행동: ... 관찰: ... (여러 번 반복) 지식 집약적 과제(예: HotpotQA, FEVER)와 의사 결정 과제(예: AlfWorld Env, WebShop)에 대한 실험에서, ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동합니다. Reflexion (Shinn & Labash 2023)은 에이전트에게 동적 메모리와 자기 반성 능력을 제공하여 추론 능력을 향상시키는 프레임워크입니다. Reflexion은 보상 모델이 간단한 이진 보상을 제공하고 작업별 행동 공간이 언어로 보완되어 복잡한 추론 단계를 가능하게 하는 ReAct의 설정을 따르는 표준 RL 설정을 갖습니다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델이 과거 출력을 피드백과 함께 명시적으로 제시함으로써 자체 출력을 개선하도록 장려합니다. MRKL (Karpas et al. 2022)은 자율 에이전트를 위한 신경 기호주의 아키텍처로, 전문 모듈의 모음을 포함하고 일반적인 목적 LLM이 최적의 전문 모듌에 문의를 라우팅하는 시스템을 제안합니다. TALM (Parisi et al. 2022)와 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을 배우기 위해 LM을 세밀하게 조정합니다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 향상시킬 수 있는지에 따라 데이터셋이 확장됩니다. ChatGPT 플러그인 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 모음은 다른 개발자들에 의해 제공될 수 있습니다(플러그인의 경우) 또는 자체 정의될 수 있습니다(함수 호출의 경우). HuggingGPT (Shen et al. 2023)은 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 모델 설명에 따라 사용 가능한 모델을 선택하고 실행 결과에 따라 응답을 요약하는 프레임워크입니다. HuggingGPT는 4단계로 구성되어 있습니다: (1) 작업 계획: LLM이 뇌로 작용하고 사용자 요청을 여러 작업으로 구문 붝합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM을 작업 구문 분석 및 계획에 이끌기 위해 소수의 예시를 사용합니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 구문 붝할 수 있으며, 사용자가 언급한 리소스의 경로를 찾을 수 있습니다. (2) 모델 선택: LLM은 전문 모델에 작업을 분배하며, 요청은 객관식 문제로 구성됩니다. LLM은 선택할 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형 기반의 필터링이 필요합니다. (3) 작업 실행: 전문 모델이 특정 작업에서 실행되고 결과를 기록합니다. AI 어시스턴트는 먼저 사용자의 요청에 직접 답변해야 하며, 그 후 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 첫인상적으로 보여줘야 합니다. 만약 추론 결과에 파일 경로가 포함되어 있다면, 사용자에게 완전한 파일 경로를 알려주어야 합니다. (4) 응답 생성: LLM은 실행 결과를 받아들이고 사용자에게 요약된 결과를 제공합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 발견 및 재료 설계 분야의 작업을 수행하기 위해 13가지 전문 설계 도구로 보강된 도메인별 예제입니다. LangChain에서 구현된 워크플로우는 ReAct 및 MRKL에서 이전에 설명된 내용을 반영하며, CoT 추론을 작업에 관련된 도구와 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 사용자 지정 프롬프트를 사용하여 필요할 때 제공된 도구를 사용하여 응답하도록 지시됩니다. 지시는 모델이 ReAct 형식을 따르도록 제안하며 - 생각, 행동, 행동 입력, 관찰. Boiko et al. (2023)는 LLM을 활용한 과학적 발견을 위한 에이전트를 조사하여 복잡한 과학 실험의 자율적 설계, 계획 및 수행을 다룹니다. 이 에이전트는 인터넷을 탐색하고 문서를 읽고 코드를 실행하고 로봇 실험 API를 호출하며 다른 LLM을 활용할 수 있습니다. Generative Agents (Park, et al. 2023)는 25개의 가상 캐릭터가 상호 작용하는 샌드박스 환경에서 LLM으로 제어되는 각 캐릭터를 실험하는 재미있는 실험입니다. Generative Agents는 상호작용 애플리케이션을 위해 인간 행동의 신뢰할 수 있는 모조품을 만들어냅니다. generative agents의 디자인은 과거 경험에 의존하여 행동하도록 하는 LLM과 메모리, 계획 및 반성 메커니즘을 결합하여 다른 에이전트와 상호 작용할 수 있도록 합니다. 메모리 스트림은 에이전트의 경험을 자연어로 기록하는 장기 메모리 모듈(외부 데이터베이스)입니다. 각 요소는 에이전트가 직접 제공한 관찰, 사건입니다. - 에이전트 간 통신은 새로운 자연어 문장을 유발할 수 있습니다. 검색 모델은 관련성, 최근성 및 중요성에 따라 에이전트의 행동을 안내하는 맥락을 제시합니다. 최근성: 최근 사건은 더 높은 점수를 받습니다. 중요성: 일상적인 것과 핵심 기억을 구별합니다. LM에게 직접 물어보세요. 관련성: 현재 상황/질의와 얼마나 관련이 있는지에 따라 결정됩니다. 반성 메커니즘은 시간이 지남에 따라 기억을 고찰하고 에이전트의 미래 행동을 안내하는 더 높은 수준의 추론을 합성합니다. 이것들은 과거 사건의 더 높은 수준의 요약입니다. Planning & Reacting: translate the reflections and the environment information into actions. Planning is essentially in order to optimize believability at the moment vs in time. Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1) Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting. Environment information is present in a tree structure. Generative Agents (Park, et al. 2023)는 상호 작용하는 가상 캐릭터를 실험하는 재미있는 실험으로, 정보 확산, 관계 기억(예: 두 에이전트가 대화 주제를 이어가는 것) 및 사회적 이벤트 조정(예: 파티를 개최하고 많은 다른 사람들을 초대)과 같은 신생 사회적 행동을 유발합니다. AutoGPT는 LLM을 주요 컨트롤러로 설정하여 자율 에이전트를 구축하는 가능성에 많은 관심을 끌었습니다. 자연어 인터페이스로 인한 신뢰성 문제가 있지만, 여전히 멋진 개념 증명 데모입니다. AutoGPT의 많은 코드는 형식 구문 분석에 관한 것입니다. AutoGPT에서 사용된 시스템 메시지는 다음과 같습니다. 여기서 {{...}}는 사용자 입력을 나타냅니다: You are {{ai-name}}, {{user-provided AI bot description}}. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. \n",
      "\n",
      "목표:\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "제약 사항:\n",
      "1. 단기 기억 용량은 약 4000 단어로 제한됩니다. 중요한 정보는 즉시 파일에 저장하십시오.\n",
      "2. 이전에 무엇을 했는지 확신이 없거나 과거 사건을 상기하려는 경우 유사한 사건을 생각하는 것이 기억에 도움이 됩니다.\n",
      "3. 사용자 지원 없음\n",
      "4. \"명령 이름\"으로 나열된 명령어만 사용\n",
      "5. 몇 분 이내에 종료되지 않는 명령에 대해 서브프로세스 사용\n",
      "\n",
      "원본 요약을 반환합니다.원본 요약을 반환합니다.원본 요약을 반환합니다.원본 요약을 반환합니다.{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"The GPT-Engineer project aims to create a repository of code by generating code based on natural language tasks. It breaks down tasks into smaller components and seeks user input for clarification.\",\n",
      "        \"reasoning\": \"The additional context provides more details about the project and its process, which can help in refining the summary.\",\n",
      "        \"plan\": \"- Incorporate the details about breaking down tasks and seeking user input\\n- Ensure the summary captures the essence of the project accurately\",\n",
      "        \"criticism\": \"The original summary was concise but lacked specific details about the project's process.\",\n",
      "        \"speak\": \"I will update the summary to include more information about how the GPT-Engineer project operates.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"update_summary\",\n",
      "        \"args\": {\n",
      "            \"new_context\": \"The GPT-Engineer project aims to create a repository of code by generating code based on natural language tasks. It breaks down tasks into smaller components and seeks user input for clarification.\"\n",
      "        }\n",
      "    }\n",
      "}{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"GPT-Engineer 프로젝트는 자연어 작업을 기반으로 코드를 생성하여 코드 저장소를 만드는 것을 목표로 합니다. 작업을 작은 구성 요소로 분해하고 사용자 입력을 통해 명확화를 시도합니다.\",\n",
      "        \"reasoning\": \"프로젝트 및 프로세스에 대한 추가적인 컨텍스트는 요약을 더 정제하는 데 도움이 될 수 있습니다.\",\n",
      "        \"plan\": \"- 작업을 분해하고 사용자 입력을 요청하는 세부 정보를 포함시킵니다.\\n- 요약이 프로젝트의 본질을 정확하게 포착하도록합니다.\",\n",
      "        \"criticism\": \"원래 요약은 간결했지만 프로젝트의 프로세스에 대한 구체적인 세부 정보가 부족했습니다.\",\n",
      "        \"speak\": \"GPT-Engineer 프로젝트가 어떻게 작동하는지에 대한 더 많은 정보를 포함하도록 요약을 업데이트하겠습니다.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"update_summary\",\n",
      "        \"args\": {\n",
      "            \"new_context\": \"GPT-Engineer 프로젝트는 자연어 작업을 기반으로 코드를 생성하여 코드 저장소를 만드는 것을 목표로 합니다. 작업을 작은 구성 요소로 분해하고 사용자 입력을 통해 명확화를 시도합니다.\"\n",
      "        }\n",
      "    }\n",
      "}{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"GPT-Engineer 프로젝트는 자연어 작업을 기반으로 코드를 생성하여 코드 저장소를 만드는 것을 목표로 합니다. 작업을 작은 구성 요소로 분해하고 사용자 입력을 통해 명확화를 시도합니다. 메인 캐릭터인 마리오는 배관공으로, 걷거나 점프할 수 있습니다. 이는 슈퍼 마리오와 같은 클래식한 플랫폼 게임으로, 캐릭터는 왼쪽에서 오른쪽으로 이동하면서 목적지로 가려고 노력합니다. 이 과정에서 많은 장애물과 적의 공격이 있습니다.\",\n",
      "        \"reasoning\": \"프로젝트의 목표와 게임의 내용을 보다 자세히 설명하여 요약을 보완하였습니다.\",\n",
      "        \"plan\": \"- 작업을 분해하고 사용자 입력을 요청하는 세부 정보를 포함시킵니다.\\n- 요약이 프로젝트의 본질을 정확하게 포착하도록합니다.\",\n",
      "        \"criticism\": \"원래 요약은 간결했지만 프로젝트의 프로세스에 대한 구체적인 세부 정보가 부족했습니다.\",\n",
      "        \"speak\": \"GPT-Engineer 프로젝트가 어떻게 작동하는지에 대한 더 많은 정보를 포함하도록 요약을 업데이트하겠습니다.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"update_summary\",\n",
      "        \"args\": {\n",
      "            \"new_context\": \"GPT-Engineer 프로젝트는 자연어 작업을 기반으로 코드를 생성하여 코드 저장소를 만드는 것을 목표로 합니다. 작업을 작은 구성 요소로 분해하고 사용자 입력을 통해 명확화를 시도합니다. 메인 캐릭터인 마리오는 배관공으로, 걷거나 점프할 수 있습니다. 이는 슈퍼 마리오와 같은 클래식한 플랫폼 게임으로, 캐릭터는 왼쪽에서 오른쪽으로 이동하면서 목적지로 가려고 노력합니다. 이 과정에서 많은 장애물과 적의 공격이 있습니다.\"\n",
      "        }\n",
      "    }\n",
      "}원래 요약은 프로젝트의 목표와 게임 내용을 잘 설명하고 있습니다. 따라서 새로운 컨텍스트가 없으므로 원래 요약을 그대로 유지하겠습니다.원래 요약은 프로젝트의 목표와 게임 내용을 잘 설명하고 있습니다. 따라서 새로운 컨텍스트가 없으므로 원래 요약을 그대로 유지하겠습니다.새로운 컨텍스트가 코드 구현과 관련이 있으므로, 요약을 다음과 같이 수정하겠습니다: 프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다.프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. 또한, Python의 경우 Python toolbelt를 사용하는 것이 좋습니다.프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. 또한, Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다.프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. 또한, Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다.프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. 또한, Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 코드를 작성할 때는 각 파일의 내용을 markdown 코드 블록 형식으로 엄격하게 작성해야 합니다. \"entrypoint\" 파일부터 시작하여 해당 파일에서 가져오는 파일들을 차례대로 작성해야 합니다. 코드 구현에 대한 자세한 내용을 포함하여 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되도록 해야 합니다. 각 클래스, 함수, 메서드의 이름을 먼저 정리하고, 그 목적에 대한 간단한 주석을 추가해야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다.프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. 또한, Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 코드를 작성할 때는 각 파일의 내용을 markdown 코드 블록 형식으로 엄격하게 작성해야 합니다. \"entrypoint\" 파일부터 시작하여 해당 파일에서 가져오는 파일들을 차례대로 작성해야 합니다. 코드 구현에 대한 자세한 내용을 포함하여 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되도록 해야 합니다. 각 클래스, 함수, 메서드의 이름을 먼저 정리하고, 그 목적에 대한 간단한 주석을 추가해야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 파일명 규칙을 준수하고, 파일에 모든 import, 타입 등이 포함되어 있는지 확인해야 합니다. 서로 다른 파일에 있는 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확신이 없는 경우에는 타당한 구현을 작성해야 합니다. 모듈 의존성이나 패키지 관리자 의존성 정의 파일을 포함해야 합니다. 마무리하기 전에 모든 아키텍처 부분이 파일에 존재하는지 두 번 확인해야 합니다.클래스를 항상 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다.프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 코드를 작성할 때는 각 파일의 내용을 markdown 코드 블록 형식으로 엄격하게 작성해야 합니다. \"entrypoint\" 파일부터 시작하여 해당 파일에서 가져오는 파일들을 차례대로 작성해야 합니다. 코드 구현에 대한 자세한 내용을 포함하여 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되도록 해야 합니다. 각 클래스, 함수, 메서드의 이름을 먼저 정리하고, 그 목적에 대한 간단한 주석을 추가해야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 파일명 규칙을 준수하고, 파일에 모든 import, 타입 등이 포함되어 있는지 확인해야 합니다. 서로 다른 파일에 있는 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확신이 없는 경우에는 타당한 구현을 작성해야 합니다. 모듈 의존성이나 패키지 관리자 의존성 정의 파일을 포함해야 합니다. 마무리하기 전에 모든 아키텍처 부분이 파일에 존재하는지 두 번 확인해야 합니다. 클래스를 항상 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python의 경우 pytest와 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다.프로젝트의 목표와 게임 내용을 설명하는 원래 요약에 더하여, 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 코드를 작성할 때는 각 파일의 내용을 markdown 코드 블록 형식으로 엄격하게 작성해야 합니다. \"entrypoint\" 파일부터 시작하여 해당 파일에서 가져오는 파일들을 차례대로 작성해야 합니다. 코드 구현에 대한 자세한 내용을 포함하여 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되도록 해야 합니다. 각 클래스, 함수, 메서드의 이름을 먼저 정리하고, 그 목적에 대한 간단한 주석을 추가해야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 파일명 규칙을 준수하고, 파일에 모든 import, 타입 등이 포함되어 있는지 확인해야 합니다. 서로 다른 파일에 있는 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확신이 없는 경우에는 타당한 구현을 작성해야 합니다. 모듈 의존성이나 패키지 관리자 의존성 정의 파일을 포함해야 합니다. 마무리하기 전에 모든 아키텍처 부분이 파일에 존재하는지 두 번 확인해야 합니다. 클래스를 항상 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python의 경우 pytest와 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 전체적인 요약에는 추가적인 내용이 없습니다.원래 요약을 보완하여, 프로젝트의 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 코드를 작성할 때는 각 파일의 내용을 markdown 코드 블록 형식으로 엄격하게 작성해야 합니다. \"entrypoint\" 파일부터 시작하여 해당 파일에서 가져오는 파일들을 차례대로 작성해야 합니다. 코드 구현에 대한 자세한 내용을 포함하여 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되도록 해야 합니다. 각 클래스, 함수, 메서드의 이름을 먼저 정리하고, 그 목적에 대한 간단한 주석을 추가해야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 파일명 규칙을 준수하고, 파일에 모든 import, 타입 등이 포함되어 있는지 확인해야 합니다. 서로 다른 파일에 있는 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확신이 없는 경우에는 타당한 구현을 작성해야 합니다. 모듈 의존성이나 패키지 관리자 의존성 정의 파일을 포함해야 합니다. 마무리하기 전에 모든 아키텍처 부분이 파일에 존재하는지 두 번 확인해야 합니다. 클래스를 항상 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python의 경우 pytest와 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 전체적인 요약에는 추가적인 내용이 없습니다.원래 요약을 보완하여, 프로젝트의 코드 구현에 대한 지침과 파일 구조에 대한 설명이 추가되었습니다. 코드는 각 클래스를 다른 파일에 작성하고, Python의 경우 requirements.txt 파일을 생성하고, NodeJS의 경우 package.json 파일을 생성해야 합니다. 함수 정의에 대한 목적을 간단히 설명하는 주석을 추가하고, 복잡한 로직에 대한 설명을 추가하는 것이 좋습니다. 코드 작성에 대한 언어별 최상의 관행을 따라야 합니다. Python의 경우 Python toolbelt를 사용하는 것이 좋습니다. pytest과 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 코드를 작성할 때는 각 파일의 내용을 markdown 코드 블록 형식으로 엄격하게 작성해야 합니다. \"entrypoint\" 파일부터 시작하여 해당 파일에서 가져오는 파일들을 차례대로 작성해야 합니다. 코드 구현에 대한 자세한 내용을 포함하여 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되도록 해야 합니다. 각 클래스, 함수, 메서드의 이름을 먼저 정리하고, 그 목적에 대한 간단한 주석을 추가해야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 파일명 규칙을 준수하고, 파일에 모든 import, 타입 등이 포함되어 있는지 확인해야 합니다. 서로 다른 파일에 있는 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확신이 없는 경우에는 타당한 구현을 작성해야 합니다. 모듈 의존성이나 패키지 관리자 의존성 정의 파일을 포함해야 합니다. 마무리하기 전에 모든 아키텍처 부분이 파일에 존재하는지 두 번 확인해야 합니다. 클래스를 항상 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python의 경우 pytest와 dataclasses를 사용하여 코드를 작성하는 것이 좋습니다. 전체적인 요약에는 추가적인 내용이 없습니다. 모든 import, 타입 등이 포함되어 있는지 확인해야 합니다. 코드가 서로 다른 파일에서 호환되도록 해야 합니다. 모든 아키텍처 부분이 파일에 존재하는지 두 번 확인해야 합니다.새로운 컨텍스트가 제공되었지만, 원래 요약에는 추가적인 내용이 필요하지 않습니다. 따라서 원래 요약을 그대로 유지하겠습니다.새로운 컨텍스트가 제공되었지만, 원래 요약에는 추가적인 내용이 필요하지 않습니다. 따라서 원래 요약을 그대로 유지하겠습니다.현재 에이전트 시스템은 LLM과 기억 및 도구와 같은 외부 구성 요소 사이의 인터페이스로 자연어를 의존하고 있습니다. 그러나 모델 출력의 신뢰성은 의문스러울 수 있으며, LLM은 형식 오류를 일으키거나 때로는 반항적인 행동(예: 지시를 거부)을 보일 수 있습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다.현재 에이전트 시스템은 LLM과 기억 및 도구와 같은 외부 구성 요소 사이의 인터페이스로 자연어를 의존하고 있습니다. 그러나 모델 출력의 신뢰성은 의문스러울 수 있으며, LLM은 형식 오류를 일으키거나 때로는 반항적인 행동(예: 지시를 거부)을 보일 수 있습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다. 최근 연구들은 LLM을 활용하여 자율 에이전트를 개발하는 방법에 대해 다양한 측면에서 탐구하고 있으며, 이를 통해 LLM의 문제 해결 능력을 향상시키는 방안을 모색하고 있습니다.현재 에이전트 시스템은 LLM과 기억 및 도구와 같은 외부 구성 요소 사이의 인터페이스로 자연어를 의존하고 있습니다. 그러나 모델 출력의 신뢰성은 의문스러울 수 있으며, LLM은 형식 오류를 일으키거나 때로는 반항적인 행동(예: 지시를 거부)을 보일 수 있습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다. 최근 연구들은 LLM을 활용하여 자율 에이전트를 개발하는 방법에 대해 다양한 측면에서 탐구하고 있으며, 이를 통해 LLM의 문제 해결 능력을 향상시키는 방안을 모색하고 있습니다. 또한, Karpas 등은 LLM, 외부 지식 소스 및 이산적 추론을 결합한 모듈식 신경 기호론 아키텍처인 MRKL 시스템에 대해 연구하고 있으며, Schick 등은 언어 모델이 스스로 도구를 사용하는 방법을 학습할 수 있는 Toolformer에 대해 연구하고 있습니다. Li 등은 도구 보조 LLM에 대한 벤치마크인 API-Bank에 대해 연구하고 있습니다.현재 에이전트 시스템은 LLM과 기억 및 도구와 같은 외부 구성 요소 사이의 인터페이스로 자연어를 의존하고 있습니다. 그러나 모델 출력의 신뢰성은 의문스러울 수 있으며, LLM은 형식 오류를 일으키거나 때로는 반항적인 행동(예: 지시를 거부)을 보일 수 있습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다. 최근 연구들은 LLM을 활용하여 자율 에이전트를 개발하는 방법에 대해 다양한 측면에서 탐구하고 있으며, 이를 통해 LLM의 문제 해결 능력을 향상시키는 방안을 모색하고 있습니다. 또한, Karpas 등은 LLM, 외부 지식 소스 및 이산적 추론을 결합한 모듈식 신경 기호론 아키텍처인 MRKL 시스템에 대해 연구하고 있으며, Schick 등은 언어 모델이 스스로 도구를 사용하는 방법을 학습할 수 있는 Toolformer에 대해 연구하고 있습니다. Li 등은 도구 보조 LLM에 대한 벤치마크인 API-Bank에 대해 연구하고 있습니다. 또한, Shen et al.은 HuggingGPT를 통해 AI 작업을 해결하는 방법에 대해 연구하고 있으며, Bran et al.은 ChemCrow를 통해 LLM에 화학 도구를 추가하는 방법을 탐구하고 있습니다. Boiko et al.은 대형 언어 모델의 신경 기호론 아키텍처를 통해 신경 기호론 아키텍처를 통해 자율적인 과학 연구 능력을 발전시키는 방법에 대해 연구하고 있습니다. Park et al.은 인간 행동의 상호작용적 시뮬라크라를 생성하는 Generative Agents에 대해 연구하고 있습니다. AutoGPT 및 GPT-Engineer는 각각 자동화된 GPT 및 GPT 엔지니어링을 위한 도구를 제공하고 있습니다.현재 에이전트 시스템은 LLM과 기억 및 도구와 같은 외부 구성 요소 사이의 인터페이스로 자연어를 의존하고 있습니다. 그러나 모델 출력의 신뢰성은 의문스러울 수 있으며, LLM은 형식 오류를 일으키거나 때로는 반항적인 행동(예: 지시를 거부)을 보일 수 있습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다. 최근 연구들은 LLM을 활용하여 자율 에이전트를 개발하는 방법에 대해 다양한 측면에서 탐구하고 있으며, 이를 통해 LLM의 문제 해결 능력을 향상시키는 방안을 모색하고 있습니다. 또한, Karpas 등은 LLM, 외부 지식 소스 및 이산적 추론을 결합한 모듈식 신경 기호론 아키텍처인 MRKL 시스템에 대해 연구하고 있으며, Schick 등은 언어 모델이 스스로 도구를 사용하는 방법을 학습할 수 있는 Toolformer에 대해 연구하고 있습니다. Li 등은 도구 보조 LLM에 대한 벤치마크인 API-Bank에 대해 연구하고 있습니다. 또한, Shen et al.은 HuggingGPT를 통해 AI 작업을 해결하는 방법에 대해 연구하고 있으며, Bran et al.은 ChemCrow를 통해 LLM에 화학 도구를 추가하는 방법을 탐구하고 있습니다. Boiko et al.은 대형 언어 모델의 신경 기호론 아키텍처를 통해 신경 기호론 아키텍처를 통해 자율적인 과학 연구 능력을 발전시키는 방법에 대해 연구하고 있습니다. Park et al.은 인간 행동의 상호작용적 시뮬라크라를 생성하는 Generative Agents에 대해 연구하고 있습니다. AutoGPT 및 GPT-Engineer는 각각 자동화된 GPT 및 GPT 엔지니어링을 위한 도구를 제공하고 있습니다."
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in Korean\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain.invoke({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d6f16",
   "metadata": {},
   "source": [
    "`print(result[\"output_text\"])`는 결과 딕셔너리 `result`에서 `'output_text'` 키에 해당하는 값을 출력합니다. 이 구문은 딕셔너리 내 특정 키의 값을 검색하고, 그 값을 콘솔에 출력하는 기본적인 Python 코드 예시입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e529f36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 또한, Joon Sung Park 등의 논문 [16]과 AutoGPT [17], GPT-Engineer [18]의 GitHub 링크를 참고할 수 있습니다. LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명하고 있습니다. 이 기사에서는 LLM 기반 자율 에이전트의 제한된 컨텍스트 용량과 관련된 도전과 문제점을 다루고 있습니다. 또한, LLM과 메모리, 도구 등 외부 구성 요소 간의 자연어 인터페이스에 의존하는 현재의 에이전트 시스템의 한계와 신뢰성 문제를 다루고 있습니다. 이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 추가적인 컨텍스트와 함께 제공됩니다.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    result[\"output_text\"]\n",
    ")  # 결과 딕셔너리에서 'output_text' 키에 해당하는 값을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893955a",
   "metadata": {},
   "source": [
    "이 함수는 `result` 딕셔너리의 `'intermediate_steps'` 키에 저장된 리스트에서 처음 세 요소를 선택하고, 이들 사이에 두 줄바꿈(`\\n\\n`)을 삽입하여 연결한 문자열을 출력합니다. 이는 중간 계산 단계나 결과를 시각적으로 구분하여 표시할 때 유용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "519754b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article discusses LLM powered autonomous agents, which are intelligent systems that can perform tasks without human intervention. The agents consist of three main components: planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes different types of memory and the use of Maximum Inner Product Search (MIPS). The tool use component is demonstrated through case studies, such as a scientific discovery agent and generative agents simulation. The article also highlights the challenges of implementing LLM powered autonomous agents.\n",
      "\n",
      "이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다.\n",
      "\n",
      "이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714ef0a",
   "metadata": {},
   "source": [
    "## 한 번의 체인으로 분할하고 요약하기\n",
    "\n",
    "편의를 위해, 우리는 긴 문서의 텍스트 분할과 요약을 단일 `AnalyzeDocumentsChain`으로 묶을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8be11",
   "metadata": {},
   "source": [
    "`AnalyzeDocumentChain` 클래스는 문서 분석 및 요약 작업을 위한 체인을 생성합니다. 이 예제에서는 `AnalyzeDocumentChain` 인스턴스를 생성하고, `combine_docs_chain`과 `text_splitter`를 인자로 전달하여 초기화합니다. 이후, 첫 번째 문서(`docs[0]`)의 `page_content`를 사용하여 문서 요약 프로세스를 실행합니다. 이 과정은 문서의 내용을 분석하고 요약하는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92a7327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article discusses LLM powered autonomous agents, which are intelligent systems that can perform tasks without human intervention. The agents consist of three main components: planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes different types of memory and the use of Maximum Inner Product Search (MIPS). The tool use component is demonstrated through case studies, such as a scientific discovery agent and generative agents simulation. The article also highlights the challenges of implementing LLM powered autonomous agents.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. 또한, Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 이전의 출력 시퀀스를 피드백과 함께 명시적으로 제시하여 모델이 스스로 개선할 수 있도록 한다. 이를 통해 모델은 피드백 시퀀스를 기반으로 자기 반성하여 더 나은 출력을 생성할 수 있다. 모델은 테스트 시에 인간 주석자와 함께 여러 라운드의 지침을 선택적으로 받을 수도 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) \"XYZ를 위한 단계.\\n1.\"과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 \"이야기 개요를 작성하십시오.\"라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. 또한, Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 이전의 출력 시퀀스를 피드백과 함께 명시적으로 제시하여 모델이 스스로 개선할 수 있도록 한다. 이를 통해 모델은 피드백 시퀀스를 기반으로 자기 반성하여 더 나은 출력을 생성할 수 있다. 모델은 테스트 시에 인간 주석자와 함께 여러 라운드의 지침을 선택적으로 받을 수도 있다. 또한, CoH는 오버피팅을 피하기 위해 사전 훈련 데이터셋의 로그 우도를 최대화하기 위한 정규화 항을 추가CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다. AD는 ED (expert distillation), source policy, RL^2와 비교하여 성능이 가까워지는 동시에 오프라인 강화 학습만 사용하므로 다른 기준선보다 훨씬 빠르게 학습한다. 또한 소스 정책의 부분 훈련 히스토리에 의존할 때, AD는 ED 기준선보다 훨씬 빠르게 개선된다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다. AD는 ED (expert distillation), source policy, RL^2와 비교하여 성능이 가까워지는 동시에 오프라인 강화 학습만 사용하므로 다른 기준선보다 훨씬 빠르게 학습한다. 또한 소스 정책의 부분 훈련 히스토리에 의존할 때, AD는 ED 기준선보다 훨씬 빠르게 개선된다. AD는 기억의 종류 중 하나인 감각 기억을 포함하여 인간의 뇌에서 다양한 종류의 기억이 있다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다. AD는 ED (expert distillation), source policy, RL^2와 비교하여 성능이 가까워지는 동시에 오프라인 강화 학습만 사용하므로 다른 기준선보다 훨씬 빠르게 학습한다. 또한 소스 정책의 부분 훈련 히스토리에 의존할 때, AD는 ED 기준선보다 훨씬 빠르게 개선된다. AD는 기억의 종류 중 하나인 감각 기억을 포함하여 인간의 뇌에서 다양한 종류의 기억이 있다. 인간의 기억은 단기 기억과 장기 기억으로 나눌 수 있으며, 장기 기억은 명시적 기억과 암시적 기억으로 구분된다. 명시적 기억은 사실과 사건에 대한 기억이며, 암시적 기억은 무의식적으로 수행되는 기술과 루틴을 포함한다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH (Locality-Sensitive Hashing)은 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY (Approximate Nearest Neighbors Oh Yeah)는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. 이는 입력 공간을 반으로 나누는 하이퍼플레인을 나타내는 비단말 노드가 있는 이진 트리의 집합이다. 각 리프는 하나의 데이터 포인트를 저장한다. ANNOY 검색은 모든 트리에서 진행되어 쿼리에 가장 가까운 반을 반복적으로 검색하고 결과를 집계한다. 이 아이디어는 KD 트리와 관련이 있지만 훨씬 확장 가능하다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH (Locality-Sensitive Hashing)은 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY (Approximate Nearest Neighbors Oh Yeah)는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. 이는 입력 공간을 반으로 나누는 하이퍼플레인을 나타내는 비단말 노드가 있는 이진 트리의 집합이다. 각 리프는 하나의 데이터 포인트를 저장한다. ANNOY 검색은 모든 트리에서 진행되어 쿼리에 가장 가까운 반을 반복적으로 검색하고 결과를 집계한다. 이 아이디어는 KD 트리와 관련이 있지만 훨씬 확장 가능하다. HNSW (Hierarchical Navigable Small World)는 소셜 네트워크의 \"육도 분리\" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 \"육도 분리\" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\\tilde{x}_i$로 양자화하는데, 이 때 내적 $\\langle q, x_i \\rangle$가 가능한한 $\\angle q, \\tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 \"육도 분리\" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\\tilde{x}_i$로 양자화하는데, 이 때 내적 $\\langle q, x_i \\rangle$가 가능한한 $\\angle q, \\tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다. LLMs에 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 \"육도 분리\" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\\tilde{x}_i$로 양자화하는데, 이 때 내적 $\\langle q, x_i \\rangle$가 가능한한 $\\angle q, \\tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다. LLMs에 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있다. MRKL (Karpas et al. 2022)는 자율 에이전트를 위한 신경 기호 아키텍처로, \"전문가\" 모듈의 컬렉션과 일반적인 목적의 LLM이라는 라우터로 구성된다. 이러한 모듈은 신경망 (예: 딥러닝 모델) 또는 심볼릭 (예: 수학 계산기, 통화 변환기, 날씨 API)일 수 있다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 \"육도 분리\" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\\tilde{x}_i$로 양자화하는데, 이 때 내적 $\\langle q, x_i \\rangle$가 가능한한 $\\angle q, \\tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다. LLMs에 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있다. MRKL (Karpas et al. 2022)는 자율 에이전트를 위한 신경 기호 아키텍처로, \"전문가\" 모듈의 컬렉션과 일반적인 목적의 LLM이라는 라우터로 구성된다. 이러한 모듈은 신경망 (예: 딥러닝 모델) 또는 심볼릭 (예: 수학 계산기, 통화 변환기, 날씨 API)일 수 있다. TALM (Tool Augmented Language Models; Parisi et al. 2022)와 Toolformer (Schick et al. 2023)은 외부 도구 API를 사용하는 방법을 학습하기 위해 LM을 세밀하게 조정한다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 개선할 수 있는지 여부에 따라 데이터셋이 확장된다. Prompt Engineering의 \"External APIs\" 섹션에서 자세한 내용을 확인할 수 있다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다:\n",
      "\n",
      "API 호출이 필요한지 여부.\n",
      "호출할 적절한 API 식별: 만족스럽지 않은 경우, LLM은 반복적으로 API 입력을 수정해야 합니다(예: 검색 엔진 API에 대한 검색 키워드 결정).\n",
      "API 결과에 기반한 응답: 결과가 만족스럽지 않은 경우, 모델은 결과를 개선하고 다시 호출할 수 있습니다.\n",
      "\n",
      "이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구했습니다. 이 에이전트는 인터넷을 검색하고 문서를 읽고 코드를 실행하며 로봇 실험 API를 호출하고 다른 LLM을 활용할 수 있습니다. 예를 들어 \"새로운 항암 약물 개발\"이라는 요청을 받았을 때, 모델은 다음과 같은 추론 단계를 제시했습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구했습니다. 이 에이전트는 인터넷을 검색하고 문서를 읽고 코드를 실행하며 로봇 실험 API를 호출하고 다른 LLM을 활용할 수 있습니다. 예를 들어 \"새로운 항암 약물 개발\"이라는 요청을 받았을 때, 모델은 다음과 같은 추론 단계를 제시했습니다. 먼저, 현재 항암 약물 개발의 최신 동향에 대해 알아보기 위해 인터넷 검색을 수행합니다. 그런 다음 특정 대상을 선택하고 이를 타겟으로 하는 화합물을 요청합니다. 화합물이 식별되면, 모델은 그 합성을 시도합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성 및 중요성에 따라 에이전트의 행동을 결정하기 위해 컨텍스트를 제공합니다. 반사 메커니즘은 시간에 걸쳐 메모리를 종합하여 미래의 행동을 안내하는 고수준의 요약입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성 및 중요성에 따라 에이전트의 행동을 결정하기 위해ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성, 목표 지향성 등을 고려하여 정보를 검색합니다. 반사 메커니즘은 에이전트가 이전에 수행한 행동을 기반으로 현재 상황을 평가하고 적절한 행동을 선택하는 데 도움을 줍니다. 이러한 기능을 통해 LLMChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형은 AI 어시스턴트가 사용자 요청을 처리하는 데 도움을 주며, 각 작업에는 특정한 인수와 관련된 속성이 있습니다. 이러한 작업 유형을 처리하기 위해서는 AI 어시스턴트가 적절한 모델을 선택하고 실행 결과를 요약하는 능력이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형은 AI 어시스턴트가 사용자 요청을 처리하는 데 도움을 주며, 각 작업에는 특정한 인수와 관련된 속성이 있습니다. 이러한 작업 유형을 처리하기 위해서는 AI 어시스턴트가 적절한 모델을 선택하고 실행 결과를 요약하는 능력이 필요합니다. AI 어시스턴트는 작업을 수행한 후 결과를 사용자에게 제시하고ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 모두 출력해야 합니다. 각 파일은 markdown 코드 블록 형식을 따라야 하며, 다음 토큰을 대체해야 합니다: FILENAME, LANG, CODE. 또한, \"entrypoint\" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 나타내는 것이 좋습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 나타내는 것이 좋습니다. 또한, pytest와 dataclasses에 대한 정보가 추가되었습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 나타내는 것이 좋습니다. 또한, pytest와 dataclasses에 대한 정보가 추가되었습니다. 이러한 새로운 정보를 고려하여 원래 요약을 수정하였습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 및 프레임워크에 적합한 파일 네이밍 규칙을 따르며, 코드가 완전하고 기능적인지 확인해야 합니다. 또한, 파일 간의 호환성을 보장하고, 필요한 경우 모듈 종속성이나 패ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 및 프레임워크에 적합한 파일 네이밍 규칙을 따르며, 코드가 완전하고 기능적인지 확인해야 합니다. 또한, 파일 간의 호환성을 보장하고, 필요한 경우 모듈 종속성이나 패키지/프로젝트로 작성된 코드를 설명하는 것이 좋습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 모델과 다른 파일 간의 호환성을 확인해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\", \"send_tweet\", \"do_nothing\", \"task_complete\"와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 모델과 다른 파일 간의 호환성을 확인해야 합니다. ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\",ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\" 등의 작업을 수행할 수 있습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\" 등의 작업을 수행할 수 있습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다. 또한, Joon Sung Park 등은 \"Generative Agents: Interactive Simulacra of Human Behavior\"에서 인간 행동의 상호작용적인 시뮬라크인 생성 에이전트에 대해 연구하였습니다. AutoGPT와 GPT-Engineer는 ChatGPT를 개선하기 위한 프로젝트입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다. 또한, Joon Sung Park 등은 \"Generative Agents: Interactive Simulacra of Human Behavior\"에서 인간 행동의 상호작용적인 시뮬라크인 생성 에이전트에 대해 연구하였습니다. AutoGPT와 GPT-Engineer는 ChatGPT를 개선하기 위한 프로젝트입니다. Prompt Engineering은 LLM의 성능을 향상시키기 위한 방법 중 하나입니다."
     ]
    }
   ],
   "source": [
    "from langchain.chains import AnalyzeDocumentChain\n",
    "\n",
    "# AnalyzeDocumentChain 인스턴스를 생성합니다. 이때, combine_docs_chain과 text_splitter를 인자로 전달합니다.\n",
    "summarize_document_chain = AnalyzeDocumentChain(\n",
    "    combine_docs_chain=chain, text_splitter=text_splitter\n",
    ")\n",
    "# 첫 번째 문서의 페이지 내용을 사용하여 문서 요약 프로세스를 실행합니다.\n",
    "summarized_result = summarize_document_chain.invoke(\n",
    "    {\"input_document\": docs[0].page_content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfb17a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, \"id\": \"id\", \"reason\": \"선택 이유에 대한 자세한 설명\"과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 \"improve_code\", \"write_tests\", \"execute_python_file\", \"generate_image\" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다. 또한, Joon Sung Park 등은 \"Generative Agents: Interactive Simulacra of Human Behavior\"에서 인간 행동의 상호작용적인 시뮬라크인 생성 에이전트에 대해 연구하였습니다. AutoGPT와 GPT-Engineer는 ChatGPT를 개선하기 위한 프로젝트입니다. Prompt Engineering은 LLM의 성능을 향상시키기 위한 방법 중 하나입니다.\n"
     ]
    }
   ],
   "source": [
    "print(summarized_result[\"output_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
